{"metadata":{"language_info":{"version":"3.5.2","file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 1","metadata":{}},{"cell_type":"markdown","source":"### Initialise\n---","metadata":{}},{"cell_type":"markdown","source":"#### import needed module","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom collections import defaultdict\nfrom ipywidgets import *\nfrom tqdm import tqdm_notebook, tqdm\nfrom aer import *\nimport pickle\nfrom copy import deepcopy\nimport os\nfrom enum import Enum\nfrom scipy.special import digamma\nfrom scipy.special import gammaln\nfrom random import random\n\nimport mmap","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#### create supporting functions\n---","metadata":{}},{"cell_type":"code","source":"def create_pairs_and_update_files(file_f,file_e,null='<NULL>'):\n    \"\"\"\n    given a french and an english file, it will loop over the pairs \n    of lines. For every occuring combination in a line, a pair is \n    added to the dict. \n    \"\"\"\n    print(\"creating pairs\")\n    counter_e = Counter()\n    counter_f = Counter()\n    for line_num, (line_f, line_e) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f), desc='Count Words', leave=True):\n        words_f = line_f.split()\n        words_e = line_e.split()\n        file_f[line_num] = words_f\n        file_e[line_num] = words_e\n        \n        for word_f in words_f:\n            counter_f[word_f] += 1\n            for word_e in words_e:\n                counter_e[word_e] += 1\n                    \n    fe_pairs = dict()\n    vocab_e = set()\n    vocab_f = set()\n    fe_pairs[('<LOW>','<LOW>')] = 1\n    for line_num, (line_f, line_e) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f), desc='Pairs', leave=True):\n        c = 0\n        for i,f in enumerate(line_f):\n            if counter_f[f] == 1:\n                file_f[line_num][i] = '<LOW>'\n                f = '<LOW>'\n            fe_pairs[(f, null)] = 1\n            if f not in vocab_f:\n                vocab_f.update([f])\n            for j,e in enumerate(line_e):\n                if not c and e != '<LOW>' and e != null and counter_e[e] == 1:\n                    file_e[line_num][j] = '<LOW>'\n                    e = '<LOW>'\n                fe_pairs[(f,e)] = 1\n                if not c and e not in vocab_e:\n                    vocab_e.update([e])\n            c += 1\n\n    return fe_pairs, vocab_f, vocab_e\n\ndef update_files(vocab_f,vocab_e,file_f,file_e,null='<NULL>'):\n    \"\"\"\n    given a french and an english file and the vocabularies, \n    it will update the files with non occuring \n    \"\"\"\n    for line_num, (line_f, line_e) in enumerate(zip(file_f,file_e)):\n        words_f = line_f.split()\n        words_e = line_e.split()\n        for i,f in enumerate(words_f):\n            if f not in vocab_f:\n                words_f[i] = '<LOW>'\n        for i,e in enumerate(words_e):\n            if e not in vocab_e:\n                words_e[i] = '<LOW>'\n        file_f[line_num] = words_f\n        file_e[line_num] = words_e","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Different enum classes, that can are used to set certain hyperparameters for training the model. ","metadata":{}},{"cell_type":"code","source":"class IBM_model(Enum):\n    I = 1\n    II = 2\n    \nclass Initialization_type(Enum):\n    uniform = 1\n    random = 2\n    modelI = 3\n    \nclass Termination_type(Enum):\n    epochs = 1\n    perplexity_convergence = 2","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Metrics calculation\n---","metadata":{}},{"cell_type":"code","source":"def calculate_perplexities(model,t,jump_dist,max_jump,file_f_train,file_e_train,file_f_val,file_e_val,calc_LL_train,use_VB=False,vb_alpha=0.1,null='<NULL>'):\n    \"\"\"\n    Given the model, it determines which perplexity calculation should be done. \n    It calculates the perplexity for both the training and the validation data. \n    \"\"\"\n    train_log_likelihood = -1 \n    train_perplexity = -1\n    if calc_LL_train:\n        train_log_likelihood, train_perplexity = calculate_perplexity(model,\n                                                                     t,\n                                                                     jump_dist,\n                                                                     max_jump,\n                                                                     file_f_train,\n                                                                     file_e_train,\n                                                                     use_VB,\n                                                                     vb_alpha,\n                                                                     null=null)        \n    val_log_likelihood, val_perplexity = calculate_perplexity(model,\n                                                             t,\n                                                             jump_dist,\n                                                             max_jump,\n                                                             file_f_val,\n                                                             file_e_val,\n                                                             use_VB,\n                                                             null=null)\n    \n    return train_log_likelihood, val_log_likelihood, train_perplexity, val_perplexity","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def calculate_perplexity(model,t,jump_dist, max_jump,file_f,file_e,use_VB=False,vb_alpha=0.1,null='<NULL>'):\n    if model == IBM_model.I:\n        log_likelihood,N = calculate_log_likelihood_modelI(t, file_f, file_e, use_VB, vb_alpha, null)\n    else:\n        log_likelihood,N = calculate_log_likelihood_modelII(t, file_f, file_e, jump_dist, max_jump, null)\n    \n    perplexity = np.exp(-1*log_likelihood/N)\n    return log_likelihood,perplexity","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Metrics_tracker:\n    def __init__(self,\n                 save_prefix, \n                 align_path, \n                 validation_truth, \n                 validation_file_f, \n                 validation_file_e,\n                 train_file_f,\n                 train_file_e,\n                 test_truth,\n                 test_file_f,\n                 test_file_e,\n                 vocab_f,\n                 vocab_e,\n                 calc_LL_train,\n                 file_enc):\n              \n        self.save_prefix = save_prefix\n        self.align_path = align_path\n        self.validation_truth = validation_truth\n        self.validation_file_f = validation_file_f\n        self.validation_file_e = validation_file_e\n        self.train_file_f = train_file_f\n        self.train_file_e = train_file_e\n        self.test_truth = test_truth\n        self.test_file_f = test_file_f\n        self.test_file_e = test_file_e\n        self.vocab_f = vocab_f\n        self.vocab_e = vocab_e\n        self.calc_LL_train = calc_LL_train\n        self.file_enc = file_enc\n        \n        # Track\n        self.val_aers = []\n        self.train_log_likelihoods = []\n        self.val_log_likelihoods = []\n        self.train_perplexities = []\n        self.val_perplexities = []\n        self.test_aer = None\n    \n    def track_metrics(self, epoch, model, t, jump_dist=None, max_jump=None, use_VB=False,vb_alpha=0.1):\n        aer = self.calculate_aer_validation(epoch, model, t, jump_dist, max_jump)\n        train_ll, val_ll, train_pp, val_pp = self.calculate_perplexities(model, t, jump_dist, max_jump, use_VB,vb_alpha)\n        \n        # Store\n        self.val_aers.append(aer)\n        self.train_log_likelihoods.append(train_ll)\n        self.val_log_likelihoods.append(val_ll)\n        self.train_perplexities.append(train_pp)\n        self.val_perplexities.append(val_pp)\n        \n    def print_last_metrics(self, epoch = None, aer=True,train_ll=True,val_ll=True,train_pp=True,val_pp=True):\n        if epoch == None:\n            epoch = len(val_aers)\n        print('Results Epoch: '+str(epoch))\n        print('====================')\n        if aer and self.val_aers:\n            print('AER:\\n\\t validation:\\t{0}'.format(self.val_aers[-1]))\n        if train_ll or val_ll:\n            print('Log Likelihood:')\n            if train_ll and self.train_log_likelihoods:\n                print('\\t train:\\t\\t{0}'.format(self.train_log_likelihoods[-1]))\n            if val_ll and self.val_log_likelihoods:\n                print('\\t validation:\\t{0}'.format(self.val_log_likelihoods[-1]))\n        if train_pp or val_pp:\n            print('Perplexity:')\n            if train_pp and self.train_perplexities:\n                print('\\t train:\\t\\t{0}'.format(self.train_perplexities[-1]))\n            if val_ll and self.val_perplexities:\n                print('\\t validation:\\t{0}'.format(self.val_perplexities[-1]))\n    \n    def save_metrics(self, file_name = 'metrics.p'):\n        metrics = {'train_log_likelihoods': self.train_log_likelihoods,\n                   'val_log_likelihoods': self.val_log_likelihoods,\n                   'train_perplexities': self.train_perplexities,\n                   'val_perplexities': self.val_perplexities,\n                   'val_aers': self.val_aers,\n                   'test_aer': self.test_aer}\n        pickle.dump(metrics, open(file_name, \"wb\"))\n    \n    def calculate_aer_validation(self, epoch, model, t, jump_dist=None, max_jump=None):\n        return self.calculate_aer(epoch, model, t, self.validation_file_f, self.validation_file_e, self.validation_truth, jump_dist, max_jump)\n        \n    def calculate_aer_test(self, epoch, model, t, jump_dist=None, max_jump=None):\n        self.test_aer = self.calculate_aer(epoch, model, t, self.test_file_f, self.test_file_e, self.test_truth, jump_dist, max_jump)\n        return self.test_aer\n    \n    def calculate_aer(self, epoch, model, t, file_f, file_e, file_truth, jump_dist, max_jump):\n        align_file = os.path.join(self.align_path,'{0}validation_epoch{1}.align'.format(self.save_prefix, epoch))\n        if model == IBM_model.I:\n            create_alignments_modelI(t, file_f, file_e, align_file, self.file_enc)\n        elif model == IBM_model.II:\n            create_alignments_modelII(t, jump_dist, max_jump, file_f, file_e, align_file, self.file_enc)\n\n        aer = test(file_truth, align_file)\n        return aer\n    \n    def calculate_perplexities(self, model, t, jump_dist, max_jump, use_VB=False,vb_alpha=0.1):\n        train_ll, val_ll, train_pp, val_pp = calculate_perplexities(model,\n                                                                    t,\n                                                                    jump_dist,\n                                                                    max_jump,\n                                                                    self.train_file_f,\n                                                                    self.train_file_e,\n                                                                    self.validation_file_f,\n                                                                    self.validation_file_e,\n                                                                    self.calc_LL_train,\n                                                                    use_VB,\n                                                                    vb_alpha)\n\n        return train_ll, val_ll, train_pp, val_pp","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# IBM I\n---","metadata":{}},{"cell_type":"code","source":"def init_params_modelI(initial_method, pairs, null='<NULL>'):\n    # Returns: t[(f,e)] - translation probabilities\n    \n    assert initial_method == Initialization_type.uniform, 'Unsupported initalization method {} for IBM model I'.format(initial_method)\n    \n    e_vocab_size = sum(1 for k,v in tqdm(pairs,  desc='Init Norm', leave=True) if v != null)\n    t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))    \n    return t","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Train\n'''\nE-step:\n    for each word j in french sentence:\n        the probability of fj|ei divided by (for t=0>m: fj|et)\n        \nM-step:\n    E[fe]/E[e]\n'''\ndef em_step_modelI(t, file_f, file_e, use_VB, alpha):\n    num_lines = len(file_f)\n    \n    # Set to zero\n    cooccurrences = defaultdict(float) # count words e and f happen together\n    total_f = defaultdict(float) # count word f happens\n    counter_f = Counter()\n    total_e = defaultdict(float) # count word e happens\n    \n    for f_sentence, e_sentence in tqdm(zip(file_f,file_e),total=num_lines,  desc='E-step', leave=True):\n        for e in e_sentence:\n            total_e[e] = 0\n            for f in f_sentence:\n                total_e[e] += t[(f,e)]\n                counter_f[f] += 1\n\n        for e in e_sentence:\n            for f in f_sentence:\n                temp = t[(f,e)] / total_e[e]\n                cooccurrences[(f,e)] += temp\n                total_f[f] += temp\n\n    for f,e in tqdm(cooccurrences.keys(),  desc='M-Step', leave=True):\n        if use_VB:\n            #theta_f|e =  exp( digamma(lambda_f|e) - digamma(sum_f' lambda(f'|e))) where lambda_f|e = E(#f-e)+alpha_f   \n            t[(f,e)] = np.exp( digamma(cooccurrences[(f,e)] + alpha) - digamma(total_f[f] + counter_f[f]*alpha))\n        else:\n            t[(f,e)] = cooccurrences[(f,e)] / total_f[f]\n        \n    return t","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def create_alignments_modelI(t, file_f, file_e, target, file_enc='utf-8'):\n    # open file to write to\n    with open(target,'w',encoding=file_enc) as tar:\n        # for each sentence in list\n        for line_num, (f_sentence,e_sentence) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f),  desc='AlignI', leave=True):\n            # for each word in sentence, find the best alignment\n            for ind_f,f in enumerate(f_sentence):\n                ind_f += 1 #0 is reserved for null\n                max_ind_e = 0 #when no alignment is found, align to zero\n                max_p = 0\n                for ind_e,e in enumerate(e_sentence):\n                    ind_e += 1 #0 is reserved for null\n                    if (f,e) in t:\n                        if t[(f,e)] > max_p:\n                            max_p = t[(f,e)]\n                            max_ind_e = ind_e\n\n                if max_ind_e != 0: # Skip null alignments\n                    # write to file. Output: sentence_line english_pos french_pos probability\n                    tar.write('%d %d %d P %f\\n'%(line_num, max_ind_e, ind_f, max_p)) ","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def calculate_log_likelihood_modelI(t, file_f, file_e, use_VB=False, vb_alpha=0.1, null='<NULL>'):\n    log_likelihood = 0\n    N = 0\n    # Set to zero\n    cooccurrences = defaultdict(float) # count words e and f happen together\n    total_f = defaultdict(float) # count word f happens\n    counter_f = Counter()\n    total_e = defaultdict(float) # count word e happens\n\n    for sentence_f, sentence_e in tqdm(zip(file_f,file_e),total=len(file_f),  desc='Calc LL', leave=True):\n        l = len(sentence_e)\n        m = len(sentence_f)\n        \n        if use_VB:\n            for e in sentence_e:\n                total_e[e] = 0\n                for f in sentence_f:\n                    if (f,e) in t:\n                        total_e[e] += t[(f,e)]\n                    counter_f[f] += 1\n\n            for e in sentence_e:\n                for f in sentence_f:\n                    if (f,e) in t:\n                        temp = t[(f,e)] / total_e[e]\n                        cooccurrences[(f,e)] += temp\n                        total_f[f] += temp\n\n        #part1\n        alignment_prob = -np.log(m*np.log(l+1)) #+ np.log(-m)\n        sentence_e = [null] + sentence_e\n        for f in sentence_f:\n            max_p = 0\n            sum_p = 0\n            for e in sentence_e:\n                if (f,e) in t:\n                    sum_p += t[(f,e)]\n                    if t[(f,e)] > max_p:\n                        max_p = t[(f,e)]        \n            N += 1\n            log_likelihood += alignment_prob + np.log(max_p)\n\n    #part2\n    if use_VB:\n        for f,e in tqdm(cooccurrences.keys(),  desc='part2_elbo', leave=True):\n            lambda_fe = cooccurrences[(f,e)] + alpha\n            exp_log_theta_fe = digamma(lambda_fe) - digamma(total_f[f] + counter_f[f]*vb_alpha)\n            log_likelihood += exp_log_theta_fe + gammaln(lambda_fe) - gammaln(vb_alpha)\n    \n    return log_likelihood, N\n","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# IBM II\n---","metadata":{}},{"cell_type":"code","source":"def init_params_modelII(initial_method, pairs, max_jump, t=None, null='<NULL>'):\n    # Returns: t[(f,e)] and jump_dist\n    \n    if t == None:\n        if initial_method == Initialization_type.uniform:\n            t = init_params_modelI(initial_method, pairs)\n        elif initial_method == Initialization_type.random:\n            t = dict(zip(pairs,[random() for x in range(len(pairs))]))\n        elif initial_method == Initialization_type.modelI:\n            # Initialize t from model I output 10 iterations\n            t = em_algorithm(model=IBM_model.I,max_epoch=10,initial_method=Initialization_type.uniform,save_pickles=False)\n\n    # Initialize jump distribution\n    jump_dist = 1. / (2 * max_jump) * np.ones([1, 2 * max_jump])\n    \n    return t, jump_dist","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Train\n\ndef em_step_modelII(t, jump_dist, max_jump, file_f, file_e, null='<NULL>'):\n    # Set to zero\n    counts_e_f = defaultdict(float) # counts words e and f happen together\n    counts_e = defaultdict(float) # counts word e happens\n    counts_jump = [0] * max_jump*2 # counts per jump between words\n    \n    num_lines = len(file_f)\n    \n    for f_sentence, e_sentence in tqdm(zip(file_f,file_e),total=num_lines,  desc='E-step', leave=True):\n        # Get lengths\n        l = len(e_sentence)\n        m = len(f_sentence)\n        f_sentence = [None] + f_sentence\n        e_sentence = [null] + e_sentence\n      \n        for i in range(1, m+1): # french\n            f = f_sentence[i]\n            den = sum(jump_dist[0, jump_func(x,i,l,m,max_jump)]*t[(f,e_sentence[x])] for x in range(0, l+1))\n            assert den != 0, 'normalization denominator is zero. i: {}, l:{}, m:{}'.format(i,l,m)\n            \n            for j in range(0, l+1): # english\n                e = e_sentence[j]\n                \n                jump_idx = jump_func(j, i, l, m, max_jump)\n                delta = t[(f,e)] * jump_dist[0, jump_idx] / den\n\n                counts_e_f[(e,f)] += delta\n                counts_e[e] += delta\n                counts_jump[jump_idx] += delta\n\n    for e,f in tqdm(counts_e_f.keys(),  desc='M-step', leave=True):\n        assert counts_e[e] != 0, 'counts_e[{}] is zero'.format(e)\n        t[(f,e)] = counts_e_f[(e,f)] / counts_e[e]\n\n    jump_den = sum(counts_jump)\n    assert jump_den != 0, 'normalization denominator for jumps is zero'\n    for i,c in enumerate(counts_jump):\n        jump_dist[0,i] = c / jump_den        \n\n    return t, jump_dist","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Jump function. From https://uva-slpl.github.io/nlp2/projects/2018/04/12/project1.html\n\ndef jump_func(i, j, m, n, max_jump):\n    \"\"\"\n    Alignment of french word j to english word i. \n    i = 0, to ,m  (we use m as in Wilker's lecture slides -- length of English sentence)\n    j = 1, to ,n  (we use n as in Wilker's lecture slides -- length of French sentence)\n    That is: a_j = i\n    with e.g. max_jump = 100\n    from[-max_jump, max_jump] to [0, 2*max_jump + 1] \n    \"\"\"\n    # We normalise j by the lenght of the French sentence and scale the result to the length of the English sentence\n    # this gives us a continuous value that is an interpolation of where we j would be in the English sentence\n    # if alignments were a linear function of the length ratio\n    jump = np.floor(i - (j * m / n)) \n    # then we collapse all jumps that are too far to the right to the maximum jump value allowed\n    if jump > max_jump:  # or we collapse all jumps that are too far to the left to the maximum (negative) jump allowed\n        jump = max_jump   \n    elif jump < -max_jump:\n        jump = -max_jump\n    # Now we shift the jump values so they start from 0\n    #  this is only necessary if you use python lists or numpy vectors to store jump probabilities\n    #  otherwise, you can use a python dict and this shifting is not required since dicts can have negative keys\n    idx = (int)(jump + max_jump)\n    if idx >= 2*max_jump: # Fix for out of bounds index\n        idx -= 1\n    return idx","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def create_alignments_modelII(t, jump_dist, max_jump, file_f, file_e, target, file_enc='utf-8', null='<NULL>'):\n    # open file to write to\n    with open(target,'w',encoding=file_enc) as tar:\n        # for each sentence in list\n        for line_num, (f_sentence,e_sentence) in tqdm(enumerate(zip(file_f,file_e)), total=len(file_f), desc='AlignII', leave=True):\n            # Get lengths\n            l = len(e_sentence)\n            m = len(f_sentence)\n            f_sentence = [None] + f_sentence\n            e_sentence = [null] + e_sentence\n\n            # for each word position in sentence, find the best alignment\n            for i in range(1, m+1): # french\n                max_p = 0\n                max_ind = 0 #when no alignment is found, align to zero\n                f = f_sentence[i]\n                for j in range(0, l+1): # english\n                    e = e_sentence[j]\n\n                    if (f,e) in t:\n                        p = t[(f,e)]*jump_dist[0, jump_func(j,i,l,m,max_jump)]\n\n                        if p >= max_p:\n                            max_p = p\n                            max_ind = j\n\n                if max_ind != 0: # Skip null alignments\n                    # write to file. Output: sentence_line english_pos french_pos probability\n                    tar.write('%d %d %d P %f\\n'%(line_num, max_ind, i, max_p)) ","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def calculate_log_likelihood_modelII(t, file_f, file_e, jump_dist, max_jump, null='<NULL>'):\n    log_likelihood = 0\n    N = 0\n    for sentence_f, sentence_e in tqdm(zip(file_f,file_e),total=len(file_f),  desc='Calc LL', leave=True):\n        l = len(sentence_e)\n        m = len(sentence_f)\n\n        sentence_e = [null] + sentence_e\n        sentence_f = [None] + sentence_f\n        \n        for i in range(1, m+1): # french\n            f = sentence_f[i]\n            max_p = 0\n            max_align_p = 0\n            for j in range(0, l+1): # english\n                e = sentence_e[j]\n                if (f,e) in t and t[(f,e)] >= max_p:\n                    max_p = t[(f,e)]\n                    max_align_p = jump_dist[0, jump_func(j,i,l,m,max_jump)]                \n            N += 1\n            log_likelihood += np.log(max_align_p) + np.log(max_p)\n    return log_likelihood, N","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Shared","metadata":{}},{"cell_type":"code","source":"def em_algorithm(model,\n                 t=None, #Only used for model II\n                 max_epoch=10, \n                 threshold=0.01,\n                 initial_method=Initialization_type.uniform, #How to initialize t\n                 terminate_method=Termination_type.epochs, \n                 train_file_f='data/training/hansards.36.2.f',\n                 train_file_e='data/training/hansards.36.2.e',\n                 validation_file_f='data/validation/dev.f',\n                 validation_file_e='data/validation/dev.e',\n                 validation_truth='data/validation/dev.wa.nonullalign',\n                 test_file_f = 'data/testing/test/test.f',\n                 test_file_e = 'data/testing/test/test.e',\n                 test_truth = 'data/testing/answers/test.wa.nonullalign',\n                 pickles_path='data/pickles/',\n                 align_path='data/alignments/',\n                 save_prefix='',\n                 save_pickles=True,\n                 use_VB=False,\n                 calc_LL_train=True,\n                 alpha=0.1, #Only used if VB is used\n                 file_enc='utf-8'):\n    \n    # test if prefix exists and correct format\n    if save_prefix != '' and save_prefix[-1]!='_':\n        save_prefix+='_'\n    \n    # read in all the files\n    with open(train_file_f, encoding=file_enc) as f:\n        train_file_f = f.readlines()\n    with open(train_file_e, encoding=file_enc) as f:\n        train_file_e = f.readlines()\n    with open(validation_file_f, encoding=file_enc) as f:\n        validation_file_f = f.readlines()\n    with open(validation_file_e, encoding=file_enc) as f:\n        validation_file_e = f.readlines()\n    with open(test_file_f, encoding=file_enc) as f:\n        test_file_f = f.readlines()\n    with open(test_file_e, encoding=file_enc) as f:\n        test_file_e = f.readlines()\n    \n    # get word pairs from corpus\n    pairs, vocab_f, vocab_e = create_pairs_and_update_files(train_file_f, train_file_e)\n    update_files(vocab_f, vocab_e, validation_file_f, validation_file_e)\n    update_files(vocab_f, vocab_e, test_file_f, test_file_e)\n\n    #initialize parameters\n    if model == IBM_model.I:\n        t = init_params_modelI(initial_method, pairs)\n    elif model == IBM_model.II:\n        # For jump function\n        max_jump = 100        \n        t, jump_dist = init_params_modelII(initial_method, pairs, max_jump, t)\n    \n    tracker = Metrics_tracker(save_prefix, \n                              align_path, \n                              validation_truth, \n                              validation_file_f, \n                              validation_file_e,\n                              train_file_f,\n                              train_file_e,\n                              test_truth,\n                              test_file_f,\n                              test_file_e,\n                              vocab_f,\n                              vocab_e,\n                              calc_LL_train,\n                              file_enc)\n    \n    # calculate initial scores before training\n    tracker.track_metrics(0, model, t, jump_dist if model == IBM_model.II else None, \n                                       max_jump if model == IBM_model.II else None,\n                                       use_VB,alpha)\n    \n    \n    #print train result\n    tracker.print_last_metrics('Init')\n        \n    # loop for max_epochs or till convergence is reached\n    for epoch in range(1,max_epoch+1):\n        print(\"start epoch: \"+str(epoch))\n        \n        # do an EM step\n        if model == IBM_model.I:\n            t = em_step_modelI(t, train_file_f, train_file_e, use_VB, alpha)\n        else:\n            t, jump_dist = em_step_modelII(t, jump_dist, max_jump, train_file_f, train_file_e)\n        \n        # create AER results and calculate the loglikelihoods/perplexity\n        tracker.track_metrics(epoch, model, t, jump_dist if model == IBM_model.II else None, \n                                               max_jump if model == IBM_model.II else None)\n        tracker.print_last_metrics(epoch)\n        \n        #store train progress\n        if save_pickles:\n            pickle.dump(t, open( os.path.join(pickles_path,'{0}t_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n            if model == IBM_model.II:\n                pickle.dump(jump_dist, open( os.path.join(pickles_path,'{0}jump_dist_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n        \n        #test for convergence\n        if terminate_method == Termination_type.perplexity_convergence:\n            if (len(tracker.train_perplexities) > 2) and (abs(tracker.train_perplexities[-2]-train_perplexity) < threshold):\n                print('Reached Convergence!')\n                break\n    \n    # Dump metrics to pickles\n    test_aer = tracker.calculate_aer_test(epoch+1,\n                                   model,\n                                   t,\n                                   jump_dist if model == IBM_model.II else None,\n                                   max_jump if model == IBM_model.II else None)\n    print('=================\\nTEST AER RESULT: {0}\\n================='.format(test_aer))\n    tracker.save_metrics(os.path.join(pickles_path,'{}metrics.p'.format(save_prefix)))\n    \n    if model == IBM_model.I:\n        return t\n    elif model == IBM_model.II:\n        return t, jump_dist","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"---\n# RUNNING THE SCRIPT","metadata":{}},{"cell_type":"markdown","source":"### RUNS\n---","metadata":{}},{"cell_type":"code","source":"t = em_algorithm(model=IBM_model.II,initial_method=Initialization_type.random, max_epoch=5, calc_LL_train=True, save_prefix='modelII_init_random3')","metadata":{},"execution_count":19,"outputs":[{"text":"Count Words:   0%|          | 422/231164 [00:00<00:54, 4204.49it/s]","name":"stderr","output_type":"stream"},{"text":"creating pairs\n","name":"stdout","output_type":"stream"},{"text":"Count Words: 100%|██████████| 231164/231164 [00:52<00:00, 4413.41it/s]\nPairs: 100%|██████████| 231164/231164 [00:56<00:00, 4056.10it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 438.85it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:11<00:00, 1206.95it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1321.67it/s]\nE-step:   0%|          | 46/231164 [00:00<08:52, 433.99it/s]","name":"stderr","output_type":"stream"},{"text":"Results Epoch: Init\n====================\nAER:\n\t validation:\t0.8997975708502024\nLog Likelihood:\n\t train:\t\t-24653513.765052523\n\t validation:\t-3862.5099861907556\nPerplexity:\n\t train:\t\t210.94938644632637\n\t validation:\t212.12096628230967\nstart epoch: 1\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [25:55<00:00, 148.60it/s]\nM-step: 100%|██████████| 11614796/11614796 [00:38<00:00, 301461.13it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 440.96it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:31<00:00, 1091.07it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1183.86it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 1\n====================\nAER:\n\t validation:\t0.4464454976303317\nLog Likelihood:\n\t train:\t\t-32826214.6950358\n\t validation:\t-5520.723935158106\nPerplexity:\n\t train:\t\t1243.5095462508118\n\t validation:\t2115.4807234307614\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 49/231164 [00:00<07:59, 482.19it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 2\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [25:19<00:00, 152.12it/s]\nM-step: 100%|██████████| 11614796/11614796 [00:26<00:00, 438851.84it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 414.72it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:25<00:00, 1127.24it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1240.72it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 2\n====================\nAER:\n\t validation:\t0.29333333333333333\nLog Likelihood:\n\t train:\t\t-26293920.338745687\n\t validation:\t-4561.857130214608\nPerplexity:\n\t train:\t\t301.1795528121136\n\t validation:\t559.5455896266167\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 46/231164 [00:00<10:38, 361.83it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 3\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [25:22<00:00, 151.82it/s]\nM-step: 100%|██████████| 11614796/11614796 [00:27<00:00, 418941.17it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 437.91it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:20<00:00, 1154.42it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1277.68it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 3\n====================\nAER:\n\t validation:\t0.26095238095238094\nLog Likelihood:\n\t train:\t\t-24241464.08600809\n\t validation:\t-4367.103713779296\nPerplexity:\n\t train:\t\t192.9002602717651\n\t validation:\t427.09621331104825\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 48/231164 [00:00<08:25, 456.82it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 4\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [26:59<00:00, 142.73it/s]\nM-step: 100%|██████████| 11614796/11614796 [00:28<00:00, 403699.40it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 481.95it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:26<00:00, 1117.87it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 592.61it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 4\n====================\nAER:\n\t validation:\t0.24688995215311005\nLog Likelihood:\n\t train:\t\t-24245801.826868773\n\t validation:\t-4483.575399630228\nPerplexity:\n\t train:\t\t193.08198214240596\n\t validation:\t501.97543207541526\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 49/231164 [00:00<07:52, 489.25it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 5\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [25:31<00:00, 150.93it/s]\nM-step: 100%|██████████| 11614796/11614796 [00:24<00:00, 472734.39it/s]\nAlignII: 100%|██████████| 37/37 [00:00<00:00, 494.82it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:18<00:00, 1161.84it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1371.80it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 5\n====================\nAER:\n\t validation:\t0.2440191387559809\nLog Likelihood:\n\t train:\t\t-24682974.45047867\n\t validation:\t-4630.668666550881\nPerplexity:\n\t train:\t\t212.30275735150735\n\t validation:\t615.5794573475407\n","name":"stdout","output_type":"stream"},{"text":"AlignII: 100%|██████████| 447/447 [00:00<00:00, 548.35it/s]\n","name":"stderr","output_type":"stream"},{"text":"=================\nTEST AER RESULT: 0.2149428252084945\n=================\n","name":"stdout","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Diana Runs","metadata":{}},{"cell_type":"code","source":"t = em_algorithm(model=IBM_model.II,initial_method=Initialization_type.random, max_epoch=5, calc_LL_train=False, save_prefix='modelII_init_random3',\n                train_file_f='NLP2/NLP2-Projects/Project1/data/training/hansards.36.2.f',\ntrain_file_e='NLP2/NLP2-Projects/Project1/data/training/hansards.36.2.e',\nvalidation_file_f='NLP2/NLP2-Projects/Project1/data/validation/dev.f',\nvalidation_file_e='NLP2/NLP2-Projects/Project1/data/validation/dev.e',\nvalidation_truth='NLP2/NLP2-Projects/Project1/data/validation/dev.wa.nonullalign',\ntest_file_f = 'NLP2/NLP2-Projects/Project1/data/testing/test/test.f',\ntest_file_e = 'NLP2/NLP2-Projects/Project1/data/testing/test/test.e',\ntest_truth = 'NLP2/NLP2-Projects/Project1/data/testing/answers/test.wa.nonullalign',\npickles_path='NLP2/NLP2-Projects/Project1/data/pickles/',\nalign_path='NLP2/NLP2-Projects/Project1/data/alignments/')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","text":"creating pairs\n","output_type":"stream"},{"name":"stderr","text":"Count Words: 100%|████████████████████| 231164/231164 [04:33<00:00, 846.04it/s]\nPairs: 100%|██████████████████████████| 231164/231164 [04:43<00:00, 814.17it/s]\nAlignII: 100%|█████████████████████████████████| 37/37 [00:00<00:00, 45.48it/s]\nCalc LL: 100%|████████████████████████████████| 37/37 [00:00<00:00, 171.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: Init\n====================\nAER:\n\t validation:\t0.896551724137931\nLog Likelihood:\n\t train:\t\t-1\n\t validation:\t-3866.965145717926\nPerplexity:\n\t train:\t\t-1\n\t validation:\t213.43574923865765\nstart epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"E-step:  93%|██████████████████████▎ | 215201/231164 [1:51:38<08:16, 32.13it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Trained  models evaluation","metadata":{}},{"cell_type":"code","source":"file_path = 'data/pickles/modelII_init_ibm1/modelII_report_ibm1_t_epoch'\nfile_path_jump = 'data/pickles/modelII_init_ibm1/modelII_report_ibm1_jump_dist_epoch'\nsave_prefix='modelII_init_ibm1_eval'\nmax_epochs = 5\nmax_jump = 100\nmodel = IBM_model.II\nuse_VB=False\nalpha=0.1\ninitial_method=Initialization_type.modelI\nfile_enc='utf-8'","metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"validation_file_f='data/validation/dev.f' \nvalidation_file_e='data/validation/dev.e'\ntrain_file_f='data/training/hansards.36.2.f'\ntrain_file_e='data/training/hansards.36.2.e'\ntest_file_f = 'data/testing/test/test.f'\ntest_file_e = 'data/testing/test/test.e'\n\nwith open(train_file_f, encoding=file_enc) as f:\n    train_file_f = f.readlines()\nwith open(train_file_e, encoding=file_enc) as f:\n    train_file_e = f.readlines()\nwith open(validation_file_f, encoding=file_enc) as f:\n    validation_file_f = f.readlines()\nwith open(validation_file_e, encoding=file_enc) as f:\n    validation_file_e = f.readlines()\nwith open(test_file_f, encoding=file_enc) as f:\n    test_file_f = f.readlines()\nwith open(test_file_e, encoding=file_enc) as f:\n    test_file_e = f.readlines()","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"pairs, vocab_f, vocab_e = create_pairs_and_update_files(train_file_f, train_file_e)\nupdate_files(vocab_f, vocab_e, validation_file_f, validation_file_e)\nupdate_files(vocab_f, vocab_e, test_file_f, test_file_e) ","metadata":{"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"Count Words:   0%|          | 982/231164 [00:00<00:46, 4903.22it/s]","output_type":"stream"},{"name":"stdout","text":"creating pairs\n","output_type":"stream"},{"name":"stderr","text":"Count Words: 100%|██████████| 231164/231164 [00:57<00:00, 4003.34it/s]\nPairs: 100%|██████████| 231164/231164 [01:02<00:00, 3680.62it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"ttemp = pickle.load(open( \"data/pickles/modelI/modelI_report_t_epoch10.p\", \"rb\" ) )\nttemp, jump_dist = init_params_modelII(initial_method, pairs, max_jump, t=ttemp)\n# ttemp = init_params_modelI(initial_method, pairs)\n\ntracker = Metrics_tracker(save_prefix=save_prefix,\n                          align_path='data/alignments/', \n                          validation_truth='data/validation/dev.wa.nonullalign', \n                          validation_file_f=validation_file_f, \n                          validation_file_e=validation_file_e,\n                          train_file_f=train_file_f,\n                          train_file_e=train_file_e,\n                          test_truth = 'data/testing/answers/test.wa.nonullalign',\n                          test_file_f=test_file_f,\n                          test_file_e=test_file_e,\n                          vocab_f=vocab_f,\n                          vocab_e=vocab_e,\n                          calc_LL_train=True,\n                          file_enc='utf-8')\ntracker.track_metrics(0, model, ttemp, jump_dist if model == IBM_model.II else None, \n                                       max_jump if model == IBM_model.II else None,\n                                       use_VB, alpha)\ntracker.print_last_metrics('init')\nfor epoch in range(1,max_epochs+1):\n    path = file_path+str(epoch)+str('.p')\n    if model == IBM_model.II:\n        path_jump = file_path_jump+str(epoch)+str('.p')\n        jump_dist = pickle.load(open( path_jump, \"rb\" ) )\n    ttemp = pickle.load(open( path, \"rb\" ) )\n    tracker.track_metrics(0, model, ttemp, jump_dist if model == IBM_model.II else None, \n                                           max_jump if model == IBM_model.II else None,\n                                           use_VB, alpha)\n    tracker.print_last_metrics(epoch)\n    ","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 457.05it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:59<00:00, 963.52it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1092.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: init\n====================\nAER:\n\t validation:\t0.3695652173913043\nLog Likelihood:\n\t train:\t\t-31301820.420033842\n\t validation:\t-5165.251025835169\nPerplexity:\n\t train:\t\t893.1796428254802\n\t validation:\t1292.081476383158\n","output_type":"stream"},{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 400.64it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:56<00:00, 976.76it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1098.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: 1\n====================\nAER:\n\t validation:\t0.246390760346487\nLog Likelihood:\n\t train:\t\t-24454024.121539105\n\t validation:\t-4272.308264231719\nPerplexity:\n\t train:\t\t202.00942723800594\n\t validation:\t374.4774446115372\n","output_type":"stream"},{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 396.88it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:53<00:00, 991.50it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 982.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: 2\n====================\nAER:\n\t validation:\t0.24472168905950098\nLog Likelihood:\n\t train:\t\t-24192558.08525369\n\t validation:\t-4246.162775960261\nPerplexity:\n\t train:\t\t190.86322745128425\n\t validation:\t361.14110697258474\n","output_type":"stream"},{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 377.27it/s]\nCalc LL: 100%|██████████| 231164/231164 [03:53<00:00, 990.01it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1155.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: 3\n====================\nAER:\n\t validation:\t0.24161073825503354\nLog Likelihood:\n\t train:\t\t-24665514.032717485\n\t validation:\t-4380.244201896991\nPerplexity:\n\t train:\t\t211.49961265806988\n\t validation:\t434.95156353554165\n","output_type":"stream"},{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 360.08it/s]\nCalc LL: 100%|██████████| 231164/231164 [04:12<00:00, 914.05it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1166.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: 4\n====================\nAER:\n\t validation:\t0.239193083573487\nLog Likelihood:\n\t train:\t\t-24958596.325552233\n\t validation:\t-4544.4270625231275\nPerplexity:\n\t train:\t\t225.39253708222182\n\t validation:\t546.180854401027\n","output_type":"stream"},{"name":"stderr","text":"AlignII: 100%|██████████| 37/37 [00:00<00:00, 380.12it/s]\nCalc LL: 100%|██████████| 231164/231164 [04:11<00:00, 919.76it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1102.59it/s]","output_type":"stream"},{"name":"stdout","text":"Results Epoch: 5\n====================\nAER:\n\t validation:\t0.23320537428023036\nLog Likelihood:\n\t train:\t\t-24986903.331971664\n\t validation:\t-4700.752832791115\nPerplexity:\n\t train:\t\t226.78176902415203\n\t validation:\t678.421072017162\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_aer = tracker.calculate_aer_test(epoch+1,\n                                      model,\n                                      ttemp,\n                                      jump_dist if model == IBM_model.II else None,\n                                      max_jump if model == IBM_model.II else None)\ntracker.save_metrics('data/pickles/modelII_init_ibm1_metrics.p')","metadata":{"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"AlignII: 100%|██████████| 447/447 [00:00<00:00, 510.85it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"jump_dist = pickle.load(open( \"data/pickles/FullModel2_t_init_uniform/modelII_init_uniform_with_LOW_jump_dist_epoch5.p\", \"rb\" ) )\nttemp = pickle.load(open( \"data/pickles/FullModel2_t_init_uniform/modelII_init_uniform_with_LOW_t_epoch5.p\", \"rb\" ) )\ntest_aer = tracker.calculate_aer_test('test',\n                                      IBM_model.II,\n                                      ttemp,\n                                      jump_dist,\n                                      100)","metadata":{},"execution_count":80,"outputs":[{"text":"AlignII: 100%|██████████| 447/447 [00:00<00:00, 540.29it/s]\n","name":"stderr","output_type":"stream"}]},{"cell_type":"code","source":"test_aer","metadata":{"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0.21582485778314087"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}