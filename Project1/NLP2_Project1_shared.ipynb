{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import needed module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from ipywidgets import *\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from aer import *\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from enum import Enum\n",
    "from scipy.special import digamma\n",
    "from random import random\n",
    "\n",
    "import mmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create supporting functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from: https://blog.nelsonliu.me/2016/07/29/progress-bars-for-python-file-reading-with-tqdm/\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "file_enc='utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignments_modelI(t, file_f, file_e, target, file_enc='utf-8'):\n",
    "    # open file to write to\n",
    "    with open(target,'w',encoding=file_enc) as tar:\n",
    "        # for each sentence in list\n",
    "        with open(file_f, encoding=file_enc) as ffil, open(file_e, encoding=file_enc) as efil:\n",
    "            for line_num, (line_f,line_e) in enumerate(zip(ffil,efil)):\n",
    "                f_sentence = line_f.split()\n",
    "                e_sentence = line_e.split()\n",
    "                # for each word in sentence, find the best alignment\n",
    "                for ind_f,f in enumerate(f_sentence):\n",
    "                    ind_f += 1 #0 is reserved for null\n",
    "                    max_ind_e = 0 #when no alignment is found, align to zero\n",
    "                    max_p = 0\n",
    "                    for ind_e,e in enumerate(e_sentence):\n",
    "                        ind_e += 1 #0 is reserved for null\n",
    "                        if (f,e) in t:\n",
    "                            if t[(f,e)] > max_p:\n",
    "                                max_p = t[(f,e)]\n",
    "                                max_ind_e = ind_e\n",
    "                    # write to file. Output: sentence_line english_pos french_pos probability\n",
    "                    tar.write('%d %d %d P %f\\n'%(line_num,max_ind_e,ind_f,max_p)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignments_modelII(t, q, file_f, file_e, target, file_enc='utf-8'):\n",
    "    # open file to write to\n",
    "    with open(target,'w',encoding=file_enc) as tar:\n",
    "        # for each sentence in list\n",
    "        with open(file_f, encoding=file_enc) as ffil, open(file_e, encoding=file_enc) as efil:\n",
    "            for line_num, (line_f,line_e) in enumerate(zip(ffil,efil)):\n",
    "                f_sentence = line_f.split()\n",
    "                e_sentence = line_e.split()\n",
    "                \n",
    "                # Get lengths\n",
    "                l = len(e_sentence)\n",
    "                m = len(f_sentence)\n",
    "            \n",
    "                # for each word position in sentence, find the best alignment\n",
    "                for i in range(0, m): # french\n",
    "                    max_p = 0\n",
    "                    max_ind = 0 #when no alignment is found, align to zero\n",
    "                    for j in range(0, l+1): # english\n",
    "                        j-= 1\n",
    "                        if j == -1:\n",
    "                            e = '<NULL>'\n",
    "                        else:    \n",
    "                            e = e_sentence[j]\n",
    "                        f = f_sentence[i]\n",
    "                        \n",
    "                        if (f,e) in t and (j,i,l,m) in q:\n",
    "                            p = q[(j,i,l,m)]*t[(f,e)]\n",
    "\n",
    "                            if p > max_p:\n",
    "                                max_p = p\n",
    "                                max_ind = j+1\n",
    "                    \n",
    "                    # write to file. Output: sentence_line english_pos french_pos probability\n",
    "                    tar.write('%d %d %d P %f\\n'%(line_num,max_ind,i+1,max_p)) \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pairs(file_f,file_e,null='<NULL>',file_enc='utf-8'):\n",
    "    fe_pairs = dict()\n",
    "    with open(file_f,encoding=file_enc) as f, open(file_e,encoding=file_enc) as e:\n",
    "        for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(file_f)):\n",
    "            for word_f in line_f.split():\n",
    "                fe_pairs[(word_f, null)] = 1\n",
    "                for word_e in line_e.split():\n",
    "                    fe_pairs[(word_f, word_e)] = 1\n",
    "    return fe_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(t,file_f,file_e,null='<NULL>',file_enc='utf-8'):\n",
    "    perplexity = 0.0\n",
    "    with open(file_f,encoding=file_enc) as f, open(file_e,encoding=file_enc) as e:\n",
    "        for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(file_f)):\n",
    "            sentence_f = line_f.split()\n",
    "            sentence_e = line_e.split()\n",
    "            sentence_e = [null] + sentence_e\n",
    "            l = len(sentence_e)\n",
    "            for f in sentence_f:\n",
    "                tmp = 0.0\n",
    "                for e in sentence_e:\n",
    "                    t_fe = 0\n",
    "                    if (f,e) in t:\n",
    "                        t_fe = t[(f,e)]\n",
    "                    tmp += t_fe/l\n",
    "                perplexity += np.log(tmp)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IBM_model(Enum):\n",
    "    I = 1\n",
    "    II = 2\n",
    "    \n",
    "class Initialization_type(Enum):\n",
    "    uniform_one = 1\n",
    "    uniform_other = 2\n",
    "    random = 3\n",
    "    modelI = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "'''\n",
    "E-step:\n",
    "    for each word j in french sentence:\n",
    "        the probability of fj|ei divided by (for t=0>m: fj|et)\n",
    "        \n",
    "M-step:\n",
    "    E[fe]/E[e]\n",
    "'''\n",
    "def em_step_modelI(t, file_f, file_e, use_VB, alpha, file_enc='utf-8'):\n",
    "    num_lines = get_num_lines(file_f)\n",
    "    \n",
    "    # Set to zero\n",
    "    cooccurrences = defaultdict(float) # number of times words e and f happen together\n",
    "    total_f = defaultdict(float) # number of times word f happens\n",
    "    total_e = defaultdict(float) # number of times word e happens\n",
    "    \n",
    "    with open(file_f,encoding=file_enc) as ffil, open(file_e,encoding=file_enc) as efil:\n",
    "        for line_f, line_e in tqdm(zip(ffil,efil),total=num_lines):\n",
    "            f_sentence = line_f.split()\n",
    "            e_sentence = line_e.split()\n",
    "            for e in e_sentence:\n",
    "                total_e[e] = 0\n",
    "                for f in f_sentence:\n",
    "                    total_e[e] += t[(f,e)]\n",
    "\n",
    "            for e in e_sentence:\n",
    "                for f in f_sentence:\n",
    "                    temp = t[(f,e)] / total_e[e]\n",
    "                    cooccurrences[(f,e)] += temp\n",
    "                    total_f[f] += temp\n",
    "\n",
    "    for f,e in tqdm(cooccurrences.keys()):\n",
    "        if use_VB:\n",
    "            t[(f,e)] = digamma(cooccurrences[(f,e)] + alpha) / digamma(total_f[f] + alpha)\n",
    "        else:\n",
    "            t[(f,e)] = cooccurrences[(f,e)] / total_f[f]\n",
    "        \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_step_modelII(t, q, file_f, file_e, file_enc='utf-8'):\n",
    "    # Set to zero\n",
    "    counts_e_f = defaultdict(float) # number of times words e and f happen together\n",
    "    counts_e = defaultdict(float) # number of times word e happens\n",
    "    counts_j_i = defaultdict(float) # number of times j (English) and i (French) align\n",
    "    counts_i = defaultdict(float) # number of i alignments\n",
    "    \n",
    "    num_lines = get_num_lines(file_f)\n",
    "    with open(file_f,encoding=file_enc) as ffil, open(file_e,encoding=file_enc) as efil:\n",
    "        for line_f, line_e in tqdm(zip(ffil,efil),total=num_lines):\n",
    "            f_sentence = line_f.split()\n",
    "            e_sentence = line_e.split()\n",
    "            \n",
    "            # Get lengths\n",
    "            l = len(e_sentence)\n",
    "            m = len(f_sentence)\n",
    "            \n",
    "            for i in range(0, m): # french\n",
    "                norm = -1\n",
    "                for j in range(0, l+1): # english\n",
    "                    j-= 1\n",
    "                    if j == -1:\n",
    "                        e = '<NULL>'\n",
    "                    else:    \n",
    "                        e = e_sentence[j]\n",
    "                    f = f_sentence[i]\n",
    "                    \n",
    "                    # Compute only once per i\n",
    "                    if norm == -1:\n",
    "                        norm = sum(q[(x-1,i,l,m)]*t[(f,e)] for x in range(0, l+1))\n",
    "                    \n",
    "                    assert norm != 0, 'norm is zero. i: {}, l:{}, m:{}'.format(i,l,m)\n",
    "                    delta = q[(j,i,l,m)]*t[(f,e)]/norm\n",
    "                    \n",
    "                    counts_e_f[(e,f)] += delta\n",
    "                    counts_e[e] += delta\n",
    "                    counts_j_i[(j,i,l,m)] += delta\n",
    "                    counts_i[(i,l,m)] += delta\n",
    "        \n",
    "        for e,f in tqdm(counts_e_f.keys()):\n",
    "            assert counts_e[e] != 0, 'counts_e[{}] is zero'.format(e)\n",
    "            t[(f,e)] = counts_e_f[(e,f)] / counts_e[e]\n",
    "        \n",
    "        for j,i,l,m in tqdm(counts_j_i.keys()):\n",
    "            assert counts_i[(i,l,m)] != 0, 'counts_i[({},{},{})] is zero'.format(i,l,m)\n",
    "            q[(j,i,l,m)] = counts_j_i[(j,i,l,m)] / counts_i[(i,l,m)]\n",
    "    \n",
    "    return t, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(model, initial_method, pairs, train_file_f=None, train_file_e=None, t=None):\n",
    "    # Returns:\n",
    "    # t[(f,e)] (model I and II)\n",
    "    # q[(j,i,l,m)] (model II only)\n",
    "    \n",
    "    if model == IBM_model.I:\n",
    "        if initial_method == Initialization_type.uniform_one:\n",
    "            # All are equally likely at the beginning, prob at one\n",
    "            t = dict(zip(pairs,[1]*len(pairs)))\n",
    "        elif initial_method == Initialization_type.uniform_other:\n",
    "            e_vocab_size = sum(1 for k,v in pairs if v != '<NULL>')\n",
    "            t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))\n",
    "        else:\n",
    "            assert True, 'Unsupported initalization method {} for IBM model I'.format(initial_method)\n",
    "        \n",
    "        return t\n",
    "    else:  \n",
    "        if initial_method == Initialization_type.uniform_other:\n",
    "            e_vocab_size = sum(1 for k,v in pairs if v != '<NULL>')\n",
    "            t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))\n",
    "        elif initial_method == Initialization_type.random:\n",
    "            t = dict(zip(pairs,[random() for x in range(len(pairs))]))\n",
    "        elif initial_method == Initialization_type.modelI and t == None:\n",
    "            # Initialize t from model I output 10 iterations\n",
    "            t,_,_,_ = em_algorithm(model=IBM_model.I,max_epoch=10,initial_method=initial_method,save_pickles=False)\n",
    "        else:\n",
    "            assert True, 'Unsupported initalization method {} for IBM model II'.format(initial_method)\n",
    "        \n",
    "        # Randomly initialize q\n",
    "        q = {}\n",
    "        with open(train_file_f,encoding=file_enc) as f, open(train_file_e,encoding=file_enc) as e:\n",
    "            for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(train_file_f)):\n",
    "                f_sentence = line_f.split()\n",
    "                e_sentence = line_e.split()\n",
    "            \n",
    "                # Get lengths\n",
    "                l = len(e_sentence)\n",
    "                m = len(f_sentence)\n",
    "\n",
    "                for i in range(0, m): # french\n",
    "                    for j in range(0, l+1): # english\n",
    "                        j-= 1\n",
    "                        q[(j,i,l,m)] = random()\n",
    "        \n",
    "        return t,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def em_algorithm(model,\n",
    "                 t=None, #Only used for model II\n",
    "                 max_epoch=10, \n",
    "                 threshold=0.01,\n",
    "                 initial_method=Initialization_type.uniform_one, #How to initialize t\n",
    "                 terminate_method='aer',\n",
    "                 train_file_f='data/training/hansards.36.2.f',\n",
    "                 train_file_e='data/training/hansards.36.2.e',\n",
    "                 validation_file_f='data/validation/dev.f',\n",
    "                 validation_file_e='data/validation/dev.e',\n",
    "                 validation_truth='data/validation/dev.wa.nonullalign',\n",
    "                 pickles_path='data/pickles/',\n",
    "                 align_path='data/alignments/',\n",
    "                 save_prefix='',\n",
    "                 save_pickles=True,\n",
    "                 use_VB=False,\n",
    "                 alpha=0.1, #Only used if VB is used\n",
    "                 file_enc='utf-8'):\n",
    "    \n",
    "    # test if prefix exists and correct format\n",
    "    if save_prefix != '' and save_prefix[-1]!='_':\n",
    "        save_prefix+='_'\n",
    "    \n",
    "    # get word pairs from corpus\n",
    "    pairs = create_pairs(train_file_f, train_file_e,file_enc=file_enc)\n",
    "    \n",
    "    #initialize parameters\n",
    "    if model == IBM_model.I:\n",
    "        t = init_params(model, initial_method, pairs)\n",
    "    else:\n",
    "        t, q = init_params(model, initial_method, pairs, train_file_f, train_file_e, t)\n",
    "    \n",
    "    # calculate initial scores before training\n",
    "    align_file = os.path.join(align_path,'{0}validation_epoch{1}.align'.format(save_prefix,0))\n",
    "    if model == IBM_model.I:\n",
    "        create_alignments_modelI(t,\n",
    "                                  validation_file_f,\n",
    "                                  validation_file_e,\n",
    "                                  align_file,\n",
    "                                  file_enc=file_enc)\n",
    "    else:\n",
    "        create_alignments_modelII(t,\n",
    "                                  q,\n",
    "                                  validation_file_f,\n",
    "                                  validation_file_e,\n",
    "                                  align_file,\n",
    "                                  file_enc=file_enc)\n",
    "\n",
    "    aer = test(validation_truth, align_file)\n",
    "    train_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n",
    "    val_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n",
    "    \n",
    "    aers = [aer]\n",
    "    train_perplexities = [train_perplexity]\n",
    "    val_perplexities = [val_perplexity]\n",
    "    #print train result\n",
    "    print('INITIAL RESULTS:\\n============\\n AER:\\n\\t validation:\\t{0}\\n PERPLEXITY:\\n\\t train:\\t\\t{1}\\n\\t validation:\\t{2}'.format(aer, train_perplexity, val_perplexity))\n",
    "        \n",
    "    # loop for max_epochs or till convergence is reached\n",
    "    for epoch in range(1,max_epoch+1):\n",
    "        print(\"start epoch: \"+str(epoch))\n",
    "        \n",
    "        # do an EM step\n",
    "        if model == IBM_model.I:\n",
    "            t = em_step_modelI(t, train_file_f, train_file_e, use_VB, alpha, file_enc=file_enc)\n",
    "        else:\n",
    "            t, q = em_step_modelII(t, q, train_file_f, train_file_e, file_enc=file_enc)\n",
    "        \n",
    "        # create AER results\n",
    "        align_file = os.path.join(align_path,'{0}validation_epoch{1}.align'.format(save_prefix,epoch))\n",
    "        if model == IBM_model.I:\n",
    "            create_alignments_modelI(t,\n",
    "                                      validation_file_f,\n",
    "                                      validation_file_e,\n",
    "                                      align_file,\n",
    "                                      file_enc=file_enc)\n",
    "        else:\n",
    "            create_alignments_modelII(t,\n",
    "                                      q,\n",
    "                                      validation_file_f,\n",
    "                                      validation_file_e,\n",
    "                                      align_file,\n",
    "                                      file_enc=file_enc)\n",
    "        \n",
    "        aer = test(validation_truth, align_file)\n",
    "        \n",
    "        # calculate the loglikelihoods\n",
    "        train_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n",
    "        val_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "        \n",
    "        #print train result\n",
    "        print('EPOCH {0}:\\n============\\n AER:\\n\\t validation:\\t{1}\\n PERPLEXITY:\\n\\t train:\\t\\t{2}\\n\\t validation:\\t{3}'.format(epoch, aer, train_perplexity, val_perplexity))\n",
    "        \n",
    "        #store train progress\n",
    "        aers.append(aer)\n",
    "        if save_pickles:\n",
    "            pickle.dump(t, open( os.path.join(pickles_path,'{0}t_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n",
    "            if model == IBM_model.II:\n",
    "                pickle.dump(q, open( os.path.join(pickles_path,'{0}q_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n",
    "        \n",
    "        #test for convergence\n",
    "        if terminate_method == 'aer':\n",
    "            if (len(aers) > 2) and (abs(aers[-2]-aer) < threshold):\n",
    "                print('Reached Convergence!')\n",
    "                break\n",
    "    \n",
    "    if model == IBM_model.I:\n",
    "        return t,aers,train_perplexities,val_perplexities\n",
    "    else:\n",
    "        return t,q,aers,train_perplexities,val_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RUNNING THE SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNS BY DIANA\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run model I\n",
    "t, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.I, max_epoch=1, save_prefix='test_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5296435925241862\n",
      "8.376959140376557e-08\n",
      "0.5296435925241862\n"
     ]
    }
   ],
   "source": [
    "t = pickle.load(open( \"data/pickles/translation_probs_10_epochs.p\", \"rb\" ) )\n",
    "print(t['36','<NULL>'])\n",
    "print(t['le', 'the'])\n",
    "\n",
    "e_vocab_size = sum(1 for k,v in t.keys() if v != '<NULL>')\n",
    "# print(e_vocab_size)\n",
    "\n",
    "for k,v in t.items():\n",
    "    if v == 1:\n",
    "        t[k] = 1/e_vocab_size\n",
    "    \n",
    "print(t['36','<NULL>'])\n",
    "print(t['le', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [04:01<00:00, 957.63it/s]\n",
      "100%|████████████████████████████████| 231164/231164 [03:40<00:00, 1050.19it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [07:30<00:00, 512.70it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [11:41<00:00, 329.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL RESULTS:\n",
      "============\n",
      " AER:\n",
      "\t validation:\t0.3975448536355052\n",
      " PERPLEXITY:\n",
      "\t train:\t\t-18375501.415776275\n",
      "\t validation:\t-18375501.415776275\n",
      "start epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 231164/231164 [9:46:02<00:00,  6.57it/s]\n",
      "100%|███████████████████████████| 11983927/11983927 [02:00<00:00, 99569.30it/s]\n",
      "100%|███████████████████████████| 13484424/13484424 [04:03<00:00, 55314.28it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [06:51<00:00, 562.42it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [06:19<00:00, 609.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "============\n",
      " AER:\n",
      "\t validation:\t0.2898961284230406\n",
      " PERPLEXITY:\n",
      "\t train:\t\t-21572445.43210116\n",
      "\t validation:\t-21572445.43210116\n",
      "start epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 231164/231164 [36:02<00:00, 106.90it/s]\n",
      "100%|██████████████████████████| 11983927/11983927 [01:32<00:00, 129387.98it/s]\n",
      "100%|███████████████████████████| 13484424/13484424 [02:32<00:00, 88189.67it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [06:27<00:00, 596.58it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [06:32<00:00, 589.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2:\n",
      "============\n",
      " AER:\n",
      "\t validation:\t0.30594900849858353\n",
      " PERPLEXITY:\n",
      "\t train:\t\t-23493670.826701827\n",
      "\t validation:\t-23493670.826701827\n",
      "start epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 231164/231164 [50:15<00:00, 76.67it/s]\n",
      "100%|███████████████████████████| 13484424/13484424 [04:28<00:00, 50145.29it/s]\n",
      "100%|█████████████████████████████████| 231164/231164 [08:39<00:00, 445.04it/s]\n",
      " 27%|█████████▎                        | 63331/231164 [02:27<06:31, 428.58it/s]"
     ]
    }
   ],
   "source": [
    "# Run model II\n",
    "t, q, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.II, t=t, save_prefix='test_model2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNS BY VICTOR\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231164/231164 [01:12<00:00, 3181.90it/s]\n",
      "100%|██████████| 231164/231164 [01:15<00:00, 3051.54it/s]\n",
      "100%|██████████| 231164/231164 [02:00<00:00, 1916.93it/s]\n",
      "100%|██████████| 231164/231164 [02:00<00:00, 1922.11it/s]\n",
      "  0%|          | 79/231164 [00:00<04:57, 777.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL RESULTS:\n",
      "============\n",
      " AER:\n",
      "\t validation:\t0.8762983947119924\n",
      " PERPLEXITY:\n",
      "\t train:\t\t-3266628.15805304\n",
      "\t validation:\t-3266628.15805304\n",
      "start epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231164/231164 [08:51<00:00, 435.21it/s]\n",
      "  0%|          | 0/11983927 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ec53b174215b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run model II\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_perplexities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_perplexities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIBM_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mII\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInitialization_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_modelII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-872a4fbbf369>\u001b[0m in \u001b[0;36mem_algorithm\u001b[0;34m(model, t, max_epoch, threshold, initial_method, terminate_method, train_file_f, train_file_e, validation_file_f, validation_file_e, validation_truth, pickles_path, align_path, save_prefix, save_pickles, use_VB, alpha, file_enc)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_step_modelI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_VB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_step_modelII\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_enc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# create AER results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-40fed946f40a>\u001b[0m in \u001b[0;36mem_step_modelII\u001b[0;34m(t, q, file_f, file_e, file_enc)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mcounts_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_e_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcounts_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'counts_e[{}] is zero'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_e_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcounts_e\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "# Run model II\n",
    "t, q, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.II, t=None, initial_method=Initialization_type.random, save_prefix='test_modelII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t, q, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.II, t=t, save_prefix='test_modelII') # You have to input t from the output of model I"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
