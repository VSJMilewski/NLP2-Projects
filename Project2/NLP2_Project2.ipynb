{"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 2","metadata":{}},{"cell_type":"markdown","source":"##### import needed packages","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm_notebook, tqdm\nfrom collections import defaultdict,Counter\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom random import shuffle","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Classes and Functions","metadata":{}},{"cell_type":"markdown","source":"## Data Manipulation","metadata":{}},{"cell_type":"markdown","source":"### Cleaning and preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess(train_data, val_data, test_data, vocab_size=10000):\n    \n    # loop over all the given files\n    for data in [train_data, val_data, test_data]:\n        # contains a source and a target file\n        for k,v in data.items():\n            tokenized_path = v[:v.find('.')] + '_tokenized.{}'.format(k)\n\n            # Tokenize \n            tokenize_command = 'perl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l {lang} < {file_path} > {output_path}'.format(\n                lang=k, file_path=v, output_path=tokenized_path)\n            print('tokenize command:\\t{}'.format(tokenize_command))\n            \n            # Lowercase\n            lowercase_path = tokenized_path[:tokenized_path.find('.')] + '_lowercased.{}'.format(k)\n            lowercase_command = 'perl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < {file_path} > {output_path}'.format(\n                file_path=tokenized_path, output_path=lowercase_path)\n            print('lowercase command:\\t{}\\n'.format(lowercase_command))\n            \n    # BPE\n    # Get vocabulary using train data\n    script_name = 'python tools/subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py'\n    args = ' --input {train_en} {train_fr} -s {num_symbols} -o {codes_file} --write-vocabulary {vocab_file}.en {vocab_file}.fr'\n    substr_index = train_data['en'].find('/')\n    vocab_file_name = train_data['en'][:substr_index] + '/vocab'\n    codes_file_name = train_data['en'][:substr_index] + '/codes.bpe'\n    learn_vocab_command = script_name + args.format(\n        train_en='data/train/train_tokenized_lowercased.en',\n        train_fr='data/train/train_tokenized_lowercased.fr',\n        num_symbols=str(vocab_size),\n        codes_file=codes_file_name,\n        vocab_file=vocab_file_name\n    )\n    print('learn vocab command:\\t{}'.format(learn_vocab_command))\n    \n    # Process all files the same way for consistency\n    script_name = 'python tools/subword-nmt/subword_nmt/apply_bpe.py'\n    for data in [train_data, val_data, test_data]:\n        for k,v in data.items():\n            args = ' -c {codes_file} --vocabulary {vocab_file}.{lang} --vocabulary-threshold 50 < {train_file}.{lang} > {train_file}_bpe.{lang}'\n            train_file_name = v[:v.find('.')] + '_tokenized_lowercased'\n            bpe_command = script_name + args.format(\n                codes_file=codes_file_name,\n                vocab_file=vocab_file_name,\n                lang=k,\n                train_file=train_file_name\n            )\n            print('bpe command:\\t{}'.format(bpe_command))","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Building dictionaries and vocabularies","metadata":{}},{"cell_type":"code","source":"UNK = '<UNK>'\nSTART = '<START>'\nEND = '<END>'","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# do we want to do the preprocessing also here?? not really right, since it is a bit to much to run every time\n# what do you mean preprocessing? tokenization, lowercasing, bpe? yes , that stuff. because i noticed u use a vocab size there as well\n# oh, no. let's do that offline, not here. yeah, but that vocab size is different. it's only for the bpe to limit its use somehow. nothing\n# to do with OUR vocab size\nclass DataProcessor():\n    def __init__(self, file_name, vocab_size):\n        self.max_sentence_length = -1\n        self.vocab_size = vocab_size\n        \n        self.file_name = file_name\n        self.sentences = self.load_data()\n        self.vocab,self.vocab_size = self.build_vocab()\n        self.w2i, self.i2w = self.build_dicts()\n        \n        \n    def load_data(self):\n        sentences = []\n        with open(self.file_name, 'r') as f:\n            for raw_line in f:\n                line = '{s} {l} {e}'.format(s=START, l=raw_line, e=END)\n                sentences.append(line.split())                \n        return sentences\n    \n    def build_dicts(self):\n        \"\"\"\n        creates lookup tables to find the index given the word \n        and the otherway around \n        \"\"\"\n        w2i = defaultdict(lambda: w2i[UNK])# would something like this work? not entirely, needs some tweaking\n        i2w = dict()\n        for i,w in enumerate(self.vocab):\n            i2w[i] = w\n            w2i[w] = i\n        return w2i, i2w\n    \n    \n    def build_vocab(self): \n        \"\"\"\n        builds a vocabulary with the most occuring words, in addition to\n        the UNK token at index 0.\n        START and END tokens are added to the vocabulary throught the\n        preprocessed sentences.\n        with vocab size none, all existing words in the data are used\n        \"\"\"\n        vocab = Counter()\n        for s in self.sentences:\n            l = len(s)\n            if len(s) > self.max_sentence_length:\n                self.max_sentence_length = l\n            for w in s:\n                vocab[w] += 1\n\n        vocab = [k for k,_ in vocab.most_common(self.vocab_size)]\n        vocab = [UNK] + vocab\n        return vocab,len(vocab)                ","metadata":{"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"### Getting data batches","metadata":{}},{"cell_type":"code","source":"# shouldn't this be a function of the DataProcessor class? I guess it could be, but don't know how to make it an iterator\ndef batch_generator(data_processor, batch_size):\n    idx = np.arange(len(data_processor.sentences))\n    \n    while True:\n        shuffle(idx)\n        batch_idx = [idx[i:i + batch_size] for i in range(0, len(idx) - (len(idx)%batch_size), batch_size)] # X[i*n_batches:(i+1)*n_batches]\n\n        for b_idx in batch_idx:\n            b_words = np.zeros([batch_size, data_processor.max_sentence_length])\n            b_positions = np.zeros([batch_size, data_processor.max_sentence_length])\n            \n            for i, bi in enumerate(b_idx):\n                sent = data_processor.sentences[bi]\n                b_words[i, :len(sent)] = np.array([data_processor.w2i[w] for w in sent])\n                b_positions[i, : len(sent)] = np.array([i for i in range(len(sent))])\n                \n            yield (torch.from_numpy(b_words).type(torch.LongTensor),\n                   torch.from_numpy(b_positions).type(torch.LongTensor))","metadata":{"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"## Sequence 2 Sequence RNN's","metadata":{}},{"cell_type":"markdown","source":"### Encoders","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, data_processor, embeddings_dim=200):\n        super().__init__()\n        self.data_processor = data_processor\n        self.word_embeddings = nn.Embedding(self.data_processor.vocab_size, embeddings_dim)\n        self.pos_embeddings = nn.Embedding(self.data_processor.max_sentence_length, embeddings_dim)\n        \n    def forward(self, word_batch, pos_batch): \n        word_emb = self.word_embeddings(word_batch)\n        pos_emb = self.pos_embeddings(pos_batch)\n        full_emb = torch.add(word_emb,pos_emb) \n        mean_emb = torch.mean(full_emb,1)\n        return full_emb, mean_emb","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Decoders","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, data_processor, hidden_dim=200, embeddings_dim=200):\n        super().__init__()\n        self.data_processor = data_processor\n        self.target_embeddings = nn.Embedding(self.data_processor.vocab_size, embeddings_dim)\n        \n#         self.attention = nn.Linear(input_dim,max_pos) \n        self.gru = nn.GRU(embeddings_dim, hidden_dim) # gru is an LSTM, and has 2 outputs\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self, encoded, hidden, batch):\n#         # attention\n#         att = self.attention(encoded)\n#         att = torch.dot(att,batch)\n        output,hidden = self.gru(att, hidden)\n        output = self.softmax(self.out(output[0]))\n        emb = self.target_embedding(output)\n        return emb, hidden\n    \n    def initword(self):\n        return self.target_embeddings(self.data_processor.w2i[START])","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Running the Code","metadata":{}},{"cell_type":"markdown","source":"## Define Hyper Parameters","metadata":{}},{"cell_type":"markdown","source":"#### files","metadata":{}},{"cell_type":"code","source":"train_data = {'en': 'data/train/train.en', 'fr': 'data/train/train.fr'}\nval_data = {'en': 'data/val/val.en', 'fr': 'data/val/val.fr'}\ntest_data = {'en': 'data/test/test_2017_flickr.en', 'fr': 'data/test/test_2017_flickr.fr'}\nsource_train_file = 'data/train/train_tokenized_lowercased_bpe.fr'\ntarget_train_file = 'data/train/train_tokenized_lowercased_bpe.en'\nsource_val_file = 'data/val/val_tokenized_lowercased_bpe.fr'\ntarget_val_file = 'data/val/val_tokenized_lowercased_bpe.en'\nsource_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.fr'\ntarget_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.en'","metadata":{"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"#### Network Parameters","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-3\nepochs = 1\nbatch_size = 16\n\nsource_vocab_size = 30000 # How can these be hyper parameters??? This is len(w2i_en) and len(w2i_fr)\ntarget_vocab_size = 30000\nhidden_dims = 256\nsource_embedding_dims = 256\ntarget_embedding_dims = 256","metadata":{"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"## process data","metadata":{}},{"cell_type":"markdown","source":"perform the preprocessing","metadata":{}},{"cell_type":"code","source":"preprocess(train_data, val_data, test_data, source_vocab_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"process data","metadata":{}},{"cell_type":"code","source":"source_processor = DataProcessor(source_train_file,source_vocab_size)\ntarget_processor = DataProcessor(target_train_file,target_vocab_size)","metadata":{"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"## setup the Network","metadata":{}},{"cell_type":"code","source":"enc = Encoder(source_processor,embeddings_dim=source_embedding_dims)\ndec = Decoder(target_processor,hidden_dim=hidden_dims,embeddings_dim=target_embedding_dims)\nparams = list(enc.parameters()) + list(dec.parameters())\nopt = Adam(params, lr=learning_rate)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## training","metadata":{}},{"cell_type":"code","source":"gen = batch_generator(source_processor, batch_size)\n\nfor epoch in range(epochs):\n    opt.zero_grad()\n    word_input_batch, pos_input_batch = next(gen)\n    enc(word_input_batch, pos_input_batch)\n    ","metadata":{"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"(tensor([[   2,    1,   61,  ...,    0,    0,    0],\n        [   2,   41,   57,  ...,    0,    0,    0],\n        [   2,    1,   70,  ...,    0,    0,    0],\n        ...,\n        [   2,   32,   71,  ...,    0,    0,    0],\n        [   2,   41,   57,  ...,    0,    0,    0],\n        [   2,   41,   64,  ...,    0,    0,    0]]), tensor([[  0,   1,   2,  ...,   0,   0,   0],\n        [  0,   1,   2,  ...,   0,   0,   0],\n        [  0,   1,   2,  ...,   0,   0,   0],\n        ...,\n        [  0,   1,   2,  ...,   0,   0,   0],\n        [  0,   1,   2,  ...,   0,   0,   0],\n        [  0,   1,   2,  ...,   0,   0,   0]]))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Don't forget to do:\n# sed -r 's/(@@ )|(@@ ?$)//g' ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}