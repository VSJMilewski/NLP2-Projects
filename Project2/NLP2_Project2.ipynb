{"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 2","metadata":{}},{"cell_type":"markdown","source":"##### import needed packages","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm_notebook, tqdm\nfrom collections import defaultdict,Counter\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom random import shuffle","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Classes and Functions","metadata":{}},{"cell_type":"markdown","source":"## Data Manipulation","metadata":{}},{"cell_type":"markdown","source":"### Cleaning and preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess(train_data, val_data, test_data, vocab_size=10000):\n    \n    # loop over all the given files\n    for data in [train_data, val_data, test_data]:\n        # contains a source and a target file\n        for k,v in data.items():\n            tokenized_path = v[:v.find('.')] + '_tokenized.{}'.format(k)\n\n            # Tokenize \n            tokenize_command = 'perl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l {lang} < {file_path} > {output_path}'.format(\n                lang=k, file_path=v, output_path=tokenized_path)\n            print('tokenize command:\\t{}'.format(tokenize_command))\n            \n            # Lowercase\n            lowercase_path = tokenized_path[:tokenized_path.find('.')] + '_lowercased.{}'.format(k)\n            lowercase_command = 'perl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < {file_path} > {output_path}'.format(\n                file_path=tokenized_path, output_path=lowercase_path)\n            print('lowercase command:\\t{}\\n'.format(lowercase_command))\n            \n    # BPE\n    # Get vocabulary using train data\n    script_name = 'python tools/subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py'\n    args = ' --input {train_en} {train_fr} -s {num_symbols} -o {codes_file} --write-vocabulary {vocab_file}.en {vocab_file}.fr'\n    substr_index = train_data['en'].find('/')\n    vocab_file_name = train_data['en'][:substr_index] + '/vocab'\n    codes_file_name = train_data['en'][:substr_index] + '/codes.bpe'\n    learn_vocab_command = script_name + args.format(\n        train_en='data/train/train_tokenized_lowercased.en',\n        train_fr='data/train/train_tokenized_lowercased.fr',\n        num_symbols=str(vocab_size),\n        codes_file=codes_file_name,\n        vocab_file=vocab_file_name\n    )\n    print('learn vocab command:\\t{}'.format(learn_vocab_command))\n    \n    # Process all files the same way for consistency\n    script_name = 'python tools/subword-nmt/subword_nmt/apply_bpe.py'\n    for data in [train_data, val_data, test_data]:\n        for k,v in data.items():\n            args = ' -c {codes_file} --vocabulary {vocab_file}.{lang} --vocabulary-threshold 50 < {train_file}.{lang} > {train_file}_bpe.{lang}'\n            train_file_name = v[:v.find('.')] + '_tokenized_lowercased'\n            bpe_command = script_name + args.format(\n                codes_file=codes_file_name,\n                vocab_file=vocab_file_name,\n                lang=k,\n                train_file=train_file_name\n            )\n            print('bpe command:\\t{}'.format(bpe_command))","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Building dictionaries and vocabularies","metadata":{}},{"cell_type":"code","source":"UNK = '<UNK>'\nSTART = '<SOS>'\nEND = '<EOS>'","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class DataProcessor():\n    def __init__(self, file_name, vocab_size):\n        self.max_sentence_length = -1\n        self.vocab_size = vocab_size\n        \n        self.file_name = file_name\n        self.sentences = self.load_data()\n        self.vocab,self.vocab_size = self.build_vocab()\n        self.w2i, self.i2w = self.build_dicts()        \n        \n    def load_data(self):\n        sentences = []\n        with open(self.file_name, 'r') as f:\n            for raw_line in f:\n                line = '{s} {l} {e}'.format(s=START, l=raw_line, e=END)\n                sentences.append(line.split())                \n        return sentences\n    \n    def build_dicts(self):\n        \"\"\"\n        creates lookup tables to find the index given the word \n        and the otherway around \n        \"\"\"\n        w2i = defaultdict(lambda: w2i[UNK])# would something like this work? not entirely, needs some tweaking\n        i2w = dict()\n        for i,w in enumerate(self.vocab):\n            i2w[i] = w\n            w2i[w] = i\n        return w2i, i2w\n    \n    \n    def build_vocab(self): \n        \"\"\"\n        builds a vocabulary with the most occuring words, in addition to\n        the UNK token at index 0.\n        START and END tokens are added to the vocabulary through the\n        preprocessed sentences.\n        with vocab size none, all existing words in the data are used\n        \"\"\"\n        vocab = Counter()\n        for s in self.sentences:\n            l = len(s)\n            if l > self.max_sentence_length:\n                self.max_sentence_length = l\n            for w in s:\n                vocab[w] += 1\n\n        vocab = [k for k,_ in vocab.most_common(self.vocab_size)]\n        vocab = [UNK] + vocab\n        return vocab,len(vocab)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Getting data batches","metadata":{}},{"cell_type":"code","source":"def batch_generator(source_processor, target_processor, batch_size):\n    idx = np.arange(len(source_processor.sentences))\n    \n    while True:\n        shuffle(idx)\n        batch_idx = [idx[i:i + batch_size] for i in range(0, len(idx) - (len(idx)%batch_size), batch_size)]\n        \n        for b_idx in batch_idx:\n            b_words_source = np.zeros([batch_size, source_processor.max_sentence_length])\n            b_positions_source = np.zeros([batch_size, source_processor.max_sentence_length])\n            b_words_target = np.zeros([batch_size, target_processor.max_sentence_length])\n            sentence_lengths_source = []\n            sentence_lengths_target = []\n            \n            for i, bi in enumerate(b_idx):\n                sent_source = source_processor.sentences[bi]\n                b_words_source[i, :len(sent_source)] = np.array([source_processor.w2i[w] for w in sent_source])\n                b_positions_source[i, :len(sent_source)] = np.array([i for i in range(len(sent_source))])\n                \n                sent_target = target_processor.sentences[bi]\n                b_words_target[i, :len(sent_target)] = np.array([target_processor.w2i[w] for w in sent_target])\n                # do we also need positions??? i've no idea\n                \n                sentence_lengths_source.append(len(sent_source))\n                sentence_lengths_target.append(len(sent_target))\n                \n            yield (torch.from_numpy(b_words_source).type(torch.LongTensor),\n                   torch.from_numpy(b_positions_source).type(torch.LongTensor),\n                   torch.from_numpy(b_words_target).type(torch.LongTensor),\n                   sentence_lengths_source,\n                   sentence_lengths_target)","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Sequence 2 Sequence RNN's","metadata":{}},{"cell_type":"markdown","source":"### Encoders","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, source_vocab_size, source_max_length, embeddings_dim):\n        super().__init__()        \n        self.word_embeddings = nn.Embedding(source_vocab_size, embeddings_dim)\n        self.pos_embeddings = nn.Embedding(source_max_length, embeddings_dim)\n        \n    def forward(self, words_batch, pos_batch):\n        words_emb = self.word_embeddings(words_batch)\n        pos_emb = self.pos_embeddings(pos_batch)\n        full_emb = torch.add(words_emb,pos_emb) \n        mean_emb = torch.mean(full_emb,1) # TODO: is it ok to grab the mean of variable length sentences? there's a lot of zero padding\n                                                # isn't this the mean over the hidden states of each word. in their slides they say this should be the input. \n                                            ### Yeah, this is the mean over hidden states and this is the decoder's input, yes\n                                                # I agree that zero paddings shouldn't be used in the calculation of the average\n                                                # but isn't the variable 'sentence length' different for each sentence?\n                                                # so 1 batch is [[<START>, w11, w12, w13, w14, <END>],\n                                                #                [<START>, w21, w22, <END>],\n                                                #                [<START>, w31, w32, w33, w34, w35, w36, w37, <END>],\n                                                #                [<START>, w41, w42, w43, w44, <END>]]\n                                                # or am i mistaken here?\n                                            ### You're right. that's exactly what 1 batch looks like.\n                                            ### Actually, the batch is padded, so it looks more like:\n                                            ###  [[<START>, w11, w12, w13, w14, <END>, <UNK>, <UNK>, <UNK>],\n                                #                [<START>, w21, w22, <END>, <UNK>, <UNK>, <UNK>, <UNK>, <UNK>],\n                                #                [<START>, w31, w32, w33, w34, w35, w36, w37, <END>],\n                                #                [<START>, w41, w42, w43, w44, <END>, <UNK>, <UNK>, <UNK>]]\n                                # can we have variable length tensors??? I've no idea\n        return full_emb, mean_emb","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Decoders","metadata":{}},{"cell_type":"code","source":"# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n\nclass Decoder(nn.Module):\n    def __init__(self, target_vocab_size, embeddings_dim, max_output_sentence_length, dropout_p=0.1): # embeddings is hidden, vocab size is output size\n        super().__init__()\n        \n        self.embedding_dim = embedding_dims\n        self.dropout_p = dropout_p\n        self.max_length = max_output_sentence_length\n        self.target_embeddings = nn.Embedding(target_vocab_size, embeddings_dim)\n        \n        self.gru = nn.GRU(embeddings_dim*2, embeddings_dim, batch_first=True) # gru is an LSTM, and has 2 outputs\n        \n        self.dropout = nn.Dropout(self.dropout_p)\n        self.out = nn.Linear(embeddings_dim, target_vocab_size)\n        \n    def forward(self, gold_words_batch, hidden_batch, stacked_encoded_words_batch):        \n        batch_size = gold_words_batch.size(0)\n        encoded_length = stacked_encoded_words_batch.size(1)\n        \n        emb = self.target_embeddings(gold_words_batch)\n        emb = self.dropout(emb)\n        \n#         print('batch size', batch_size)\n#         print('emb', emb.shape)\n#         print('stacked', stacked_encoded_words_batch.shape)\n#         print('hidden batch', hidden_batch.shape)\n        \n        # attention        \n        alphas = torch.zeros(batch_size, encoded_length)\n        \n        for i in range(batch_size):\n            for j in range(encoded_length):\n                # Do the batched version of dot product (https://discuss.pytorch.org/t/dot-product-batch-wise/9746)\n                alphas[:,j] = torch.bmm(hidden_batch.view(batch_size, 1, self.embedding_dim),\n                                        stacked_encoded_words_batch[:,j,:].view(batch_size, self.embedding_dim, 1)).view(batch_size)\n        \n        alphas = F.softmax(alphas, dim=1).view(batch_size, 1, encoded_length)\n        \n#         print('alphas', alphas.shape)\n        \n        # context is weights x hidden states from encoder\n        context = torch.bmm(alphas, stacked_encoded_words_batch)     \n        \n#         print('context', context.shape)\n        \n        # we have to concat context + emb        \n        input = torch.cat((emb, context), 2)\n#         print('input', input.shape)\n        \n        gru_output, hidden = self.gru(input, hidden_batch.view(1, batch_size, self.embedding_dim))\n        \n#         print('gru',gru_output.shape)\n        output = self.out(gru_output)\n#         print('output', output.shape)\n        output = F.log_softmax(output, dim=1)\n#         print('after log softmax', output.shape)\n        \n        return output, hidden","metadata":{"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"markdown","source":"# Running the Code","metadata":{}},{"cell_type":"markdown","source":"## Define Hyper Parameters","metadata":{}},{"cell_type":"markdown","source":"#### files","metadata":{}},{"cell_type":"code","source":"# Raw\ntrain_data = {'en': 'data/train/train.en', 'fr': 'data/train/train.fr'}\nval_data = {'en': 'data/val/val.en', 'fr': 'data/val/val.fr'}\ntest_data = {'en': 'data/test/test_2017_flickr.en', 'fr': 'data/test/test_2017_flickr.fr'}\n\n# Preprocessed\nsource_train_file = 'data/train/train_tokenized_lowercased_bpe.fr'\ntarget_train_file = 'data/train/train_tokenized_lowercased_bpe.en'\nsource_val_file = 'data/val/val_tokenized_lowercased_bpe.fr'\ntarget_val_file = 'data/val/val_tokenized_lowercased_bpe.en'\nsource_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.fr'\ntarget_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.en'","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Network Parameters","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-3\niterations = 1\nbatch_size = 16\n\nsource_vocab_size = 30000\ntarget_vocab_size = 30000\nhidden_dims = 128\nembedding_dims = 128","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## process data","metadata":{}},{"cell_type":"markdown","source":"perform the preprocessing","metadata":{}},{"cell_type":"code","source":"preprocess(train_data, val_data, test_data, source_vocab_size)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"tokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/train/train.en > data/train/train_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/train/train_tokenized.en > data/train/train_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/train/train.fr > data/train/train_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/train/train_tokenized.fr > data/train/train_tokenized_lowercased.fr\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/val/val.en > data/val/val_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/val/val_tokenized.en > data/val/val_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/val/val.fr > data/val/val_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/val/val_tokenized.fr > data/val/val_tokenized_lowercased.fr\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/test/test_2017_flickr.en > data/test/test_2017_flickr_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/test/test_2017_flickr_tokenized.en > data/test/test_2017_flickr_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/test/test_2017_flickr.fr > data/test/test_2017_flickr_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/test/test_2017_flickr_tokenized.fr > data/test/test_2017_flickr_tokenized_lowercased.fr\n\nlearn vocab command:\tpython tools/subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input data/train/train_tokenized_lowercased.en data/train/train_tokenized_lowercased.fr -s 30000 -o data/codes.bpe --write-vocabulary data/vocab.en data/vocab.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/train/train_tokenized_lowercased.en > data/train/train_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/train/train_tokenized_lowercased.fr > data/train/train_tokenized_lowercased_bpe.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/val/val_tokenized_lowercased.en > data/val/val_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/val/val_tokenized_lowercased.fr > data/val/val_tokenized_lowercased_bpe.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/test/test_2017_flickr_tokenized_lowercased.en > data/test/test_2017_flickr_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/test/test_2017_flickr_tokenized_lowercased.fr > data/test/test_2017_flickr_tokenized_lowercased_bpe.fr\n","output_type":"stream"}]},{"cell_type":"markdown","source":"prepare data for the model","metadata":{}},{"cell_type":"code","source":"source_processor = DataProcessor(source_train_file, source_vocab_size)\ntarget_processor = DataProcessor(target_train_file, target_vocab_size)","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## setup the Network","metadata":{}},{"cell_type":"code","source":"encoder = Encoder(source_processor.vocab_size,\n                  source_processor.max_sentence_length, \n                  embeddings_dim=embedding_dims)\ndecoder = Decoder(target_processor.vocab_size, \n                  embeddings_dim=embedding_dims, \n                  max_output_sentence_length=target_processor.max_sentence_length)\n\nopt_encoder = Adam(encoder.parameters(), lr=learning_rate)\nopt_decoder = Adam(decoder.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"markdown","source":"## training","metadata":{}},{"cell_type":"code","source":"n_training_examples = len(source_processor.sentences)\ngen = batch_generator(source_processor, target_processor, batch_size)\n# batches_per_epoch = int(np.ceil(n_training_examples / batch_size)) # should this be floor?\n\nfor it in range(iterations):    \n    output = 0\n    loss = nn.NLLLoss()\n    \n    opt_encoder.zero_grad()\n    opt_decoder.zero_grad()\n\n    words_batch_source, pos_batch_source, words_batch_target, sentence_lengths_source, sentence_lengths_target = next(gen)\n\n    # Encode\n    all_embs, mean_emb = encoder(words_batch_source, pos_batch_source)\n\n    # Decode\n    hidden_state_batch = mean_emb\n\n    for w_idx in range(target_processor.max_sentence_length):\n#             if w_idx < sentence_lengths_target[w_idx]: # This is wrong!!!\n        prediction, hidden_state_batch = decoder(\n            words_batch_target[:,w_idx].view(-1,1), \n            hidden_state_batch,\n            all_embs)\n        \n#         print(prediction.shape)\n#         print(words_batch_target[:,w_idx].shape)\n        \n        output += loss(prediction.view(batch_size, target_processor.vocab_size), \n                       words_batch_target[:,w_idx])\n    \n    output.backward()\n\n    opt_encoder.step()\n    opt_decoder.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Don't forget to do:\n# sed -r 's/(@@ )|(@@ ?$)//g' ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}