{"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 1","metadata":{}},{"cell_type":"markdown","source":"### Initialise\n---","metadata":{}},{"cell_type":"markdown","source":"#### import needed module","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom collections import defaultdict\nfrom ipywidgets import *\nfrom tqdm import tqdm_notebook, tqdm\nfrom aer import *\nimport pickle\nfrom copy import deepcopy\nimport os\nfrom enum import Enum\nfrom scipy.special import digamma\nfrom random import random\n\nimport mmap","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### create supporting functions\n---","metadata":{}},{"cell_type":"code","source":"# from: https://blog.nelsonliu.me/2016/07/29/progress-bars-for-python-file-reading-with-tqdm/\ndef get_num_lines(file_path):\n    fp = open(file_path, \"r+\")\n    buf = mmap.mmap(fp.fileno(), 0)\n    lines = 0\n    while buf.readline():\n        lines += 1\n    return lines\n\nfile_enc='utf-8'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_alignments(t, file_f, file_e, target, file_enc='utf-8'):\n    # open file to write to\n    with open(target,'w',encoding=file_enc) as tar:\n        # for each sentence in list\n        with open(file_f, encoding=file_enc) as ffil, open(file_e, encoding=file_enc) as efil:\n            for line_num, (line_f,line_e) in enumerate(zip(ffil,efil)):\n                f_sentence = line_f.split()\n                e_sentence = line_e.split()\n                # for each word in sentence, find the best allignment\n                for ind_f,f in enumerate(f_sentence):\n                    ind_f += 1 #0 is reserved for null\n                    max_ind_e = 0 #when no allignment is found, align to zero\n                    max_p = 0\n                    for ind_e,e in enumerate(e_sentence):\n                        ind_e += 1 #0 is reserved for null\n                        if (f,e) in t:\n                            if t[(f,e)] > max_p:\n                                max_p = t[(f,e)]\n                                max_ind_e = ind_e\n                    # write to file. Output: sentence_line english_pos french_pos probability\n                    tar.write('%d %d %d P %f\\n'%(line_num,max_ind_e,ind_f,max_p)) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_pairs(file_f,file_e,null='<NULL>',file_enc='utf-8'):\n    fe_pairs = dict()\n    with open(file_f,encoding=file_enc) as f, open(file_e,encoding=file_enc) as e:\n        for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(file_f)):\n            for word_f in line_f.split():\n                fe_pairs[(word_f, null)] = 1\n                for word_e in line_e.split():\n                    fe_pairs[(word_f, word_e)] = 1\n    return fe_pairs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_perplexity(t,file_f,file_e,null='<NULL>',file_enc='utf-8'):\n    perplexity = 0.0\n    with open(file_f,encoding=file_enc) as f, open(file_e,encoding=file_enc) as e:\n        for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(file_f)):\n            sentence_f = line_f.split()\n            sentence_e = line_e.split()\n            sentence_e = [null] + sentence_e\n            l = len(sentence_e)\n            for f in sentence_f:\n                tmp = 0.0\n                for e in sentence_e:\n                    t_fe = 0\n                    if (f,e) in t:\n                        t_fe = t[(f,e)]\n                    tmp += t_fe/l\n                perplexity += np.log(tmp)\n    return perplexity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class IBM_model(Enum):\n    I = 1\n    II = 2\n    \nclass Initialization_type(Enum):\n    uniform_one = 1\n    uniform_other = 2\n    random = 3\n    modelI = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IBM 1\n---","metadata":{}},{"cell_type":"code","source":"# Train\n'''\nE-step:\n    for each word j in french sentence:\n        the probability of fj|ei divided by (for t=0>m: fj|et)\n        \nM-step:\n    E[fe]/E[e]\n'''\ndef em_step_modelI(t, file_f, file_e, use_VB, alpha, file_enc='utf-8'):\n    num_lines = get_num_lines(file_f)\n    \n    # Set to zero\n    cooccurrences = defaultdict(float) # number of times words e and f happen together\n    total_f = defaultdict(float) # number of times word f happens\n    total_e = defaultdict(float) # number of times word e happens\n    \n    with open(file_f,encoding=file_enc) as ffil, open(file_e,encoding=file_enc) as efil:\n        for line_f, line_e in tqdm(zip(ffil,efil),total=num_lines):\n            f_sentence = line_f.split()\n            e_sentence = line_e.split()\n            for e in e_sentence:\n                total_e[e] = 0\n                for f in f_sentence:\n                    total_e[e] += t[(f,e)]\n\n            for e in e_sentence:\n                for f in f_sentence:\n                    temp = t[(f,e)] / total_e[e]\n                    cooccurrences[(f,e)] += temp\n                    total_f[f] += temp\n\n    for f,e in tqdm(cooccurrences.keys()):\n        if use_VB:\n            t[(f,e)] = digamma(cooccurrences[(f,e)] + alpha) / digamma(total_f[f] + alpha)\n        else:\n            t[(f,e)] = cooccurrences[(f,e)] / total_f[f]\n        \n    return t","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def em_step_modelII(t, q, file_f, file_e, file_enc='utf-8'):\n    # Set to zero\n    counts_e_f = defaultdict(float) # number of times words e and f happen together\n    counts_e = defaultdict(float) # number of times word e happens\n    counts_j_i = defaultdict(float) # number of times j (English) and i (French) align\n    counts_i = defaultdict(float) # number of i alignments\n    \n    num_lines = get_num_lines(file_f)\n    with open(file_f,encoding=file_enc) as ffil, open(file_e,encoding=file_enc) as efil:\n        for line_f, line_e in tqdm(zip(ffil,efil),total=num_lines):\n            f_sentence = line_f.split()\n            e_sentence = line_e.split()\n            \n            # Get lengths\n            l = len(e_sentence)\n            m = len(f_sentence)\n            \n            for i in range(0, m): # french\n                norm = sum(q[(x-1,i,l,m)]*t[(f,e)] for x in range(0, l+1))\n                assert norm != 0, 'norm is zero. i: {}, l:{}, m:{}'.format(i,l,m)\n                for j in range(0, l+1): # english\n                    j-= 1\n                    if j == -1:\n                        e = '<NULL>'\n                    else:    \n                        e = e_sentence[j]\n                    f = f_sentence[i]\n                    \n                    delta = q[(j,i,l,m)]*t[(f,e)]/norm\n                    \n                    counts_e_f[(e,f)] += delta\n                    counts_e[e] += delta\n                    counts_j_i[(j,i,l,m)] += delta\n                    counts_i[(i,l,m)] += delta\n                    \n            break ### REMOVE ###\n        \n        for f,e in tqdm(counts_e_f.keys()):\n            assert counts_e[e] != 0, 'counts_e[{}] is zero'.format(e)\n            t[(f,e)] = counts_e_f[(e,f)] / counts_e[e]\n        \n        for j,i,l,m in tqdm(counts_j_i.keys()):\n            assert counts_i[(i,l,m)] != 0, 'counts_i[(i,l,m)] is zero'.format(e)\n            q[(j,i,l,m)] = counts_j_i[(j,i,l,m)] / counts_i[(i,l,m)]\n    \n    return t, q","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_params(model, initial_method, pairs, train_file_f=None, train_file_e=None, t=None):\n    # Returns:\n    # t[(f,e)] (model I and II)\n    # q[(j,i,l,m)] (model II only)\n    \n    if model == IBM_model.I:\n        if initial_method == Initialization_type.uniform_one:\n            # All are equally likely at the beginning, prob at one\n            t = dict(zip(pairs,[1]*len(pairs)))\n        elif initial_method == Initialization_type.uniform_other:\n            e_vocab_size = sum(1 for k,v in pairs if v != '<NULL>')\n            t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))\n        else:\n            assert True, 'Unsupported initalization method {} for IBM model I'.format(initial_method)\n        \n        return t\n    else:  \n        if initial_method == Initialization_type.uniform_other:\n            e_vocab_size = sum(1 for k,v in pairs if v != '<NULL>')\n            t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))\n        elif initial_method == Initialization_type.random:\n            t = dict(zip(pairs,[random() for x in range(len(pairs))]))\n        elif initial_method == Initialization_type.modelI and t == None:\n            # Initialize t from model I output 10 iterations\n            t,_,_,_ = em_algorithm(model=IBM_model.I,max_epoch=10,initial_method=initial_method,save_pickles=False)\n        else:\n            assert True, 'Unsupported initalization method {} for IBM model II'.format(initial_method)\n        \n        # Randomly initialize q\n        q = {}\n        with open(train_file_f,encoding=file_enc) as f, open(train_file_e,encoding=file_enc) as e:\n            for line_f, line_e in tqdm(zip(f,e),total=get_num_lines(train_file_f)):\n                f_sentence = line_f.split()\n                e_sentence = line_e.split()\n            \n                # Get lengths\n                l = len(e_sentence)\n                m = len(f_sentence)\n\n                for i in range(0, m): # french\n                    for j in range(0, l+1): # english\n                        j-= 1\n                        q[(j,i,l,m)] = random()\n        \n        return t,q","metadata":{},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def em_algorithm(model,\n                 t=None, #Only used for model II\n                 max_epoch=10, \n                 threshold=0.01,\n                 initial_method=Initialization_type.uniform_one, #How to initialize t\n                 terminate_method='aer',\n                 train_file_f='data/training/hansards.36.2.f',\n                 train_file_e='data/training/hansards.36.2.e',\n                 validation_file_f='data/validation/dev.f',\n                 validation_file_e='data/validation/dev.e',\n                 validation_truth='data/validation/dev.wa.nonullalign',\n                 pickles_path='data/pickles/',\n                 align_path='data/alignments/',\n                 save_prefix='',\n                 save_pickles=True,\n                 use_VB=False,\n                 alpha=0.1, #Only used if VB is used\n                 file_enc='utf-8'):\n    \n    # test if prefix exists and correct format\n    if save_prefix != '' and save_prefix[-1]!='_':\n        save_prefix+='_'\n    \n    # get word pairs from corpus\n    pairs = create_pairs(train_file_f, train_file_e,file_enc=file_enc)\n    \n    #initialize parameters\n    if model == IBM_model.I:\n        t = init_params(model, initial_method, pairs)\n    else:\n        t, q = init_params(model, initial_method, pairs, train_file_f, train_file_e, t)\n    \n    # calculate initial scores before training\n    align_file = os.path.join(align_path,'{0}validation_epoch{1}.align'.format(save_prefix,0))\n    create_alignments(t,\n                      validation_file_f,\n                      validation_file_e,\n                      align_file,\n                      file_enc=file_enc)\n\n    aer = test(validation_truth, align_file)\n    train_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n    val_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n    \n    aers = [aer]\n    train_perplexities = [train_perplexity]\n    val_perplexities = [val_perplexity]\n    #print train result\n    print('INITIAL RESULTS:\\n============\\n AER:\\n\\t validation:\\t{0}\\n PERPLEXITY:\\n\\t train:\\t\\t{1}\\n\\t validation:\\t{2}'.format(aer, train_perplexity, val_perplexity))\n        \n    # loop for max_epochs or till convergence is reached\n    for epoch in range(1,max_epoch+1):\n        print(\"start epoch: \"+str(epoch))\n        \n        # do an EM step\n        if model == IBM_model.I:\n            t = em_step_modelI(t, train_file_f, train_file_e, use_VB, alpha, file_enc=file_enc)\n        else:\n            t, q = em_step_modelII(t, q, train_file_f, train_file_e)\n        \n        # create AER results\n        align_file = os.path.join(align_path,'{0}validation_epoch{1}.align'.format(save_prefix,epoch))\n        create_alignments(t,\n                          validation_file_f,\n                          validation_file_e,\n                          align_file,\n                          file_enc=file_enc)\n        aer = test(validation_truth, align_file)\n        \n        # calculate the loglikelihoods\n        train_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n        val_perplexity = calculate_perplexity(t,train_file_f,train_file_e,file_enc=file_enc)\n        train_perplexities.append(train_perplexity)\n        val_perplexities.append(val_perplexity)\n        \n        #print train result\n        print('EPOCH {0}:\\n============\\n AER:\\n\\t validation:\\t{1}\\n PERPLEXITY:\\n\\t train:\\t\\t{2}\\n\\t validation:\\t{3}'.format(epoch, aer, train_perplexity, val_perplexity))\n        \n        #store train progress\n        aers.append(aer)\n        if save_pickles:\n            pickle.dump(t, open( os.path.join(pickles_path,'{0}t_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n        \n        #test for convergence\n        if terminate_method == 'aer':\n            if (len(aers) > 2) and (abs(aers[-2]-aer) < threshold):\n                print('Reached Convergence!')\n                break\n    \n    if model == IBM_model.I:\n        return t,aers,train_perplexities,val_perplexities\n    else:\n        return t,q,aers,train_perplexities,val_perplexities","metadata":{},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"---\n# RUNNING THE SCRIPT","metadata":{}},{"cell_type":"markdown","source":"### RUNS BY DIANA\n---","metadata":{}},{"cell_type":"code","source":"# Run model I\nt, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.I, max_epoch=1, save_prefix='test_modelI')","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"t = pickle.load(open( \"data/pickles/translation_probs_10_epochs.p\", \"rb\" ) )","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Run model II\nt, q, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.II, t=t, max_epoch=1, save_prefix='test_modelII')","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|█████████████████████████████████| 231164/231164 [04:36<00:00, 835.26it/s]\n100%|█████████████████████████████████| 231164/231164 [03:57<00:00, 971.46it/s]\n  0%|                                               | 0/231164 [00:00<?, ?it/s]C:\\Users\\Diana\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n  app.launch_new_instance()\n 84%|███████████████████████████▋     | 193932/231164 [08:36<01:39, 375.49it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"### RUNS BY VICTOR\n---","metadata":{}},{"cell_type":"code","source":"t, aers, train_perplexities, val_perplexities = em_algorithm(model=IBM_model.I, max_epoch=100, use_VB=True, save_prefix='with_VB')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}