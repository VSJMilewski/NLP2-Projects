{"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 1","metadata":{}},{"cell_type":"markdown","source":"### Initialise\n---","metadata":{}},{"cell_type":"markdown","source":"#### import needed module","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom collections import defaultdict\nfrom ipywidgets import *\nfrom tqdm import tqdm_notebook, tqdm\nfrom aer import *\nimport pickle\nfrom copy import deepcopy\nimport os\nfrom enum import Enum\nfrom scipy.special import digamma\nfrom random import random\n\nimport mmap","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### create supporting functions\n---","metadata":{}},{"cell_type":"code","source":"def create_pairs_and_update_files(file_f,file_e,null='<NULL>'):\n    \"\"\"\n    given a french and an english file, it will loop over the pairs \n    of lines. For every occuring combination in a line, a pair is \n    added to the dict. \n    \"\"\"\n    print(\"creating pairs\")\n    counter_e = Counter()\n    counter_f = Counter()\n    for line_num, (line_f, line_e) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f), desc='Count Words', leave=True):\n        words_f = line_f.split()\n        words_e = line_e.split()\n        file_f[line_num] = words_f\n        file_e[line_num] = words_e\n        \n        for word_f in words_f:\n            counter_f[word_f] += 1\n            for word_e in words_e:\n                counter_e[word_e] += 1\n                    \n    fe_pairs = dict()\n    vocab_e = set()\n    vocab_f = set()\n    fe_pairs[('<LOW>','<LOW>')] = 1\n    for line_num, (line_f, line_e) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f), desc='Pairs', leave=True):\n        c = 0\n        for i,f in enumerate(line_f):\n            if counter_f[f] == 1:\n                file_f[line_num][i] = '<LOW>'\n                f = '<LOW>'\n            fe_pairs[(f, null)] = 1\n            if f not in vocab_f:\n                vocab_f.update([f])\n            for j,e in enumerate(line_e):\n                if not c and e != '<LOW>' and e != null and counter_e[e] == 1:\n                    file_e[line_num][j] = '<LOW>'\n                    e = '<LOW>'\n                fe_pairs[(f,e)] = 1\n                if not c and e not in vocab_e:\n                    vocab_e.update([e])\n            c += 1\n\n    return fe_pairs, vocab_f, vocab_e\n\ndef update_files(vocab_f,vocab_e,file_f,file_e,null='<NULL>'):\n    \"\"\"\n    given a french and an english file and the vocabularies, \n    it will update the files with non occuring \n    \"\"\"\n    for line_num, (line_f, line_e) in enumerate(zip(file_f,file_e)):\n        words_f = line_f.split()\n        words_e = line_e.split()\n        for i,f in enumerate(words_f):\n            if f not in vocab_f:\n                words_f[i] = '<LOW>'\n        for i,e in enumerate(words_e):\n            if e not in vocab_e:\n                words_e[i] = '<LOW>'\n        file_f[line_num] = words_f\n        file_e[line_num] = words_e","metadata":{},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Different enum classes, that can are used to set certain hyperparameters for training the model. ","metadata":{}},{"cell_type":"code","source":"class IBM_model(Enum):\n    I = 1\n    II = 2\n    \nclass Initialization_type(Enum):\n    uniform = 1\n    random = 2\n    modelI = 3\n    \nclass Termination_type(Enum):\n    epochs = 1\n    perplexity_convergence = 2","metadata":{},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Metrics calculation\n---","metadata":{}},{"cell_type":"code","source":"def calculate_perplexities(model,t,jump_dist,max_jump,file_f_train,file_e_train,file_f_val,file_e_val,calc_LL_train,null='<NULL>'):\n    \"\"\"\n    Given the model, it determines which perplexity calculation should be done. \n    It calculates the perplexity for both the training and the validation data. \n    \"\"\"\n    train_log_likelihood = -1 \n    train_perplexity = -1\n    if calc_LL_train:\n        train_log_likelihood, train_perplexity = calculate_perplexity(model,\n                                                                     t,\n                                                                     jump_dist,\n                                                                     max_jump,\n                                                                     file_f_train,\n                                                                     file_e_train,\n                                                                     null=null)        \n    val_log_likelihood, val_perplexity = calculate_perplexity(model,\n                                                             t,\n                                                             jump_dist,\n                                                             max_jump,\n                                                             file_f_val,\n                                                             file_e_val,\n                                                             null=null)\n    \n    return train_log_likelihood, val_log_likelihood, train_perplexity, val_perplexity","metadata":{},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def calculate_perplexity(model,t,jump_dist, max_jump,file_f,file_e,null='<NULL>'):\n    if model == IBM_model.I:\n        log_likelihood,N = calculate_log_likelihood_modelI(t, file_f, file_e, null)\n    else:\n        log_likelihood,N = calculate_log_likelihood_modelII(t, file_f, file_e, jump_dist, max_jump, null)\n    \n    perplexity = np.exp(-1*log_likelihood/N)\n    return log_likelihood,perplexity","metadata":{},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Metrics_tracker:\n    def __init__(self,\n                 save_prefix, \n                 align_path, \n                 validation_truth, \n                 validation_file_f, \n                 validation_file_e,\n                 train_file_f,\n                 train_file_e,\n                 test_truth,\n                 test_file_f,\n                 test_file_e,\n                 vocab_f,\n                 vocab_e,\n                 calc_LL_train,\n                 file_enc):\n                \n        self.save_prefix = save_prefix\n        self.align_path = align_path\n        self.validation_truth = validation_truth\n        self.validation_file_f = validation_file_f\n        self.validation_file_e = validation_file_e\n        self.train_file_f = train_file_f\n        self.train_file_e = train_file_e\n        self.test_truth = test_truth\n        self.test_file_f = test_file_f\n        self.test_file_e = test_file_e\n        self.vocab_f = vocab_f\n        self.vocab_e = vocab_e\n        self.calc_LL_train = calc_LL_train\n        self.file_enc = file_enc\n        \n        # Track\n        self.val_aers = []\n        self.train_log_likelihoods = []\n        self.val_log_likelihoods = []\n        self.train_perplexities = []\n        self.val_perplexities = []\n        self.test_aer = None\n    \n    def track_metrics(self, epoch, model, t, jump_dist=None, max_jump=None):\n        aer = self.calculate_aer_validation(epoch, model, t, jump_dist, max_jump)\n        train_ll, val_ll, train_pp, val_pp = self.calculate_perplexities(model, t, jump_dist, max_jump)\n        \n        # Store\n        self.val_aers.append(aer)\n        self.train_log_likelihoods.append(train_ll)\n        self.val_log_likelihoods.append(val_ll)\n        self.train_perplexities.append(train_pp)\n        self.val_perplexities.append(val_pp)\n        \n    def print_last_metrics(self, epoch = None, aer=True,train_ll=True,val_ll=True,train_pp=True,val_pp=True):\n        if epoch == None:\n            epoch = len(val_aers)\n        print('Results Epoch: '+str(epoch))\n        print('====================')\n        if aer and self.val_aers:\n            print('AER:\\n\\t validation:\\t{0}'.format(self.val_aers[-1]))\n        if train_ll or val_ll:\n            print('Log Likelihood:')\n            if train_ll and self.train_log_likelihoods:\n                print('\\t train:\\t\\t{0}'.format(self.train_log_likelihoods[-1]))\n            if val_ll and self.val_log_likelihoods:\n                print('\\t validation:\\t{0}'.format(self.val_log_likelihoods[-1]))\n        if train_pp or val_pp:\n            print('Perplexity:')\n            if train_pp and self.train_perplexities:\n                print('\\t train:\\t\\t{0}'.format(self.train_perplexities[-1]))\n            if val_ll and self.val_perplexities:\n                print('\\t validation:\\t{0}'.format(self.val_perplexities[-1]))\n    \n    def save_metrics(self, file_name = 'metrics.p'):\n        metrics = {'train_log_likelihoods': self.train_log_likelihoods,\n                   'val_log_likelihoods': self.val_log_likelihoods,\n                   'train_perplexities': self.train_perplexities,\n                   'val_perplexities': self.val_perplexities,\n                   'val_aers': self.val_aers,\n                   'test_aer': self.test_aer}\n        pickle.dump(metrics, open(file_name, \"wb\"))\n    \n    def calculate_aer_validation(self, epoch, model, t, jump_dist=None, max_jump=None):\n        return self.calculate_aer(epoch, model, t, self.validation_file_f, self.validation_file_e, self.validation_truth, jump_dist, max_jump)\n        \n    def calculate_aer_test(self, epoch, model, t, jump_dist=None, max_jump=None):\n        self.test_aer = self.calculate_aer(epoch, model, t, self.test_file_f, self.test_file_e, self.test_truth, jump_dist, max_jump)\n        return self.test_aer\n    \n    def calculate_aer(self, epoch, model, t, file_f, file_e, file_truth, jump_dist, max_jump):\n        align_file = os.path.join(self.align_path,'{0}validation_epoch{1}.align'.format(self.save_prefix, epoch))\n        if model == IBM_model.I:\n            create_alignments_modelI(t, file_f, file_e, align_file, self.file_enc)\n        elif model == IBM_model.II:\n            create_alignments_modelII(t, jump_dist, max_jump, file_f, file_e, align_file, self.file_enc)\n\n        aer = test(file_truth, align_file)\n        return aer\n    \n    def calculate_perplexities(self, model, t, jump_dist, max_jump):\n        train_ll, val_ll, train_pp, val_pp = calculate_perplexities(model,\n                                                                    t,\n                                                                    jump_dist,\n                                                                    max_jump,\n                                                                    self.train_file_f,\n                                                                    self.train_file_e,\n                                                                    self.validation_file_f,\n                                                                    self.validation_file_e,\n                                                                    self.calc_LL_train)\n\n        return train_ll, val_ll, train_pp, val_pp","metadata":{},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# IBM I\n---","metadata":{}},{"cell_type":"code","source":"def init_params_modelI(initial_method, pairs, null='<NULL>'):\n    # Returns: t[(f,e)] - translation probabilities\n    \n    assert initial_method == Initialization_type.uniform, 'Unsupported initalization method {} for IBM model I'.format(initial_method)\n    \n    e_vocab_size = sum(1 for k,v in tqdm(pairs,  desc='Init Norm', leave=True) if v != null)\n    t = dict(zip(pairs,[1.0/e_vocab_size]*len(pairs)))    \n    return t","metadata":{},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Train\n'''\nE-step:\n    for each word j in french sentence:\n        the probability of fj|ei divided by (for t=0>m: fj|et)\n        \nM-step:\n    E[fe]/E[e]\n'''\ndef em_step_modelI(t, file_f, file_e, use_VB, alpha):\n    num_lines = len(file_f)\n    \n    # Set to zero\n    cooccurrences = defaultdict(float) # count words e and f happen together\n    total_f = defaultdict(float) # count word f happens\n    counter_f = Counter()\n    total_e = defaultdict(float) # count word e happens\n    \n    for f_sentence, e_sentence in tqdm(zip(file_f,file_e),total=num_lines,  desc='E-step', leave=True):\n        for e in e_sentence:\n            total_e[e] = 0\n            for f in f_sentence:\n                total_e[e] += t[(f,e)]\n                counter_f[f] += 1\n\n        for e in e_sentence:\n            for f in f_sentence:\n                temp = t[(f,e)] / total_e[e]\n                cooccurrences[(f,e)] += temp\n                total_f[f] += temp\n\n    for f,e in tqdm(cooccurrences.keys(),  desc='M-Step', leave=True):\n        if use_VB:\n            #theta_f|e =  exp( digamma(lambda_f|e) - digamma(sum_f' lambda(f'|e))) where lambda_f|e = E(#f-e)+alpha_f   \n            t[(f,e)] = np.exp( digamma(cooccurrences[(f,e)] + alpha) - digamma(total_f[f] + counter_f[f]*alpha))\n        else:\n            t[(f,e)] = cooccurrences[(f,e)] / total_f[f]\n        \n    return t","metadata":{},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def create_alignments_modelI(t, file_f, file_e, target, file_enc='utf-8'):\n    # open file to write to\n    with open(target,'w',encoding=file_enc) as tar:\n        # for each sentence in list\n        for line_num, (f_sentence,e_sentence) in tqdm(enumerate(zip(file_f,file_e)),total=len(file_f),  desc='AlignI', leave=True):\n            # for each word in sentence, find the best alignment\n            for ind_f,f in enumerate(f_sentence):\n                ind_f += 1 #0 is reserved for null\n                max_ind_e = 0 #when no alignment is found, align to zero\n                max_p = 0\n                for ind_e,e in enumerate(e_sentence):\n                    ind_e += 1 #0 is reserved for null\n                    if (f,e) in t:\n                        if t[(f,e)] > max_p:\n                            max_p = t[(f,e)]\n                            max_ind_e = ind_e\n\n                if max_ind_e != 0: # Skip null alignments\n                    # write to file. Output: sentence_line english_pos french_pos probability\n                    tar.write('%d %d %d P %f\\n'%(line_num, max_ind_e, ind_f, max_p)) ","metadata":{},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def calculate_log_likelihood_modelI(t, file_f, file_e, null='<NULL>'):\n    log_likelihood = 0\n    N = 0\n    for sentence_f, sentence_e in tqdm(zip(file_f,file_e),total=len(file_f), desc='Calc LL', leave=True):\n        l = len(sentence_e)\n        m = len(sentence_f)\n        sentence_e = [null] + sentence_e\n        \n        #                np.log(-m * np.log(l + 1))\n        \n        alignment_prob = -np.log(m*np.log(l+1)) #+ np.log(-m)\n\n        for f in sentence_f:\n            max_p = 0\n            for e in sentence_e:\n                if (f,e) in t and t[(f,e)] > max_p:\n                    max_p = t[(f,e)]        \n            N += 1\n            log_likelihood += alignment_prob + np.log(max_p)\n    return log_likelihood, N","metadata":{},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# IBM II\n---","metadata":{}},{"cell_type":"code","source":"def init_params_modelII(initial_method, pairs, max_jump, t=None, null='<NULL>'):\n    # Returns: t[(f,e)] and jump_dist\n    \n    if t == None:\n        if initial_method == Initialization_type.uniform:\n            t = init_params_modelI(initial_method, pairs)\n        elif initial_method == Initialization_type.random:\n            t = dict(zip(pairs,[random() for x in range(len(pairs))]))\n        elif initial_method == Initialization_type.modelI:\n            # Initialize t from model I output 10 iterations\n            t = em_algorithm(model=IBM_model.I,max_epoch=10,initial_method=Initialization_type.uniform,save_pickles=False)\n\n    # Initialize jump distribution\n    jump_dist = 1. / (2 * max_jump) * np.ones([1, 2 * max_jump])\n    \n    return t, jump_dist","metadata":{},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Train\n\ndef em_step_modelII(t, jump_dist, max_jump, file_f, file_e, null='<NULL>'):\n    # Set to zero\n    counts_e_f = defaultdict(float) # counts words e and f happen together\n    counts_e = defaultdict(float) # counts word e happens\n    counts_jump = [0] * max_jump*2 # counts per jump between words\n    \n    num_lines = len(file_f)\n    \n    for f_sentence, e_sentence in tqdm(zip(file_f,file_e),total=num_lines,  desc='E-step', leave=True):\n        # Get lengths\n        l = len(e_sentence)\n        m = len(f_sentence)\n        f_sentence = [None] + f_sentence\n        e_sentence = [null] + e_sentence\n      \n        for i in range(1, m+1): # french\n            f = f_sentence[i]\n            den = sum(jump_dist[0, jump_func(x,i,l,m,max_jump)]*t[(f,e_sentence[x])] for x in range(0, l+1))\n            assert den != 0, 'normalization denominator is zero. i: {}, l:{}, m:{}'.format(i,l,m)\n            \n            for j in range(0, l+1): # english\n                e = e_sentence[j]\n                \n                jump_idx = jump_func(j, i, l, m, max_jump)\n                delta = t[(f,e)] * jump_dist[0, jump_idx] / den\n\n                counts_e_f[(e,f)] += delta\n                counts_e[e] += delta\n                counts_jump[jump_idx] += delta\n\n    for e,f in tqdm(counts_e_f.keys(),  desc='M-step', leave=True):\n        assert counts_e[e] != 0, 'counts_e[{}] is zero'.format(e)\n        t[(f,e)] = counts_e_f[(e,f)] / counts_e[e]\n\n    jump_den = sum(counts_jump)\n    assert jump_den != 0, 'normalization denominator for jumps is zero'\n    for i,c in enumerate(counts_jump):\n        jump_dist[0,i] = c / jump_den        \n\n    return t, jump_dist","metadata":{},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Jump function. From https://uva-slpl.github.io/nlp2/projects/2018/04/12/project1.html\n\ndef jump_func(i, j, m, n, max_jump):\n    \"\"\"\n    Alignment of french word j to english word i. \n    i = 0, to ,m  (we use m as in Wilker's lecture slides -- length of English sentence)\n    j = 1, to ,n  (we use n as in Wilker's lecture slides -- length of French sentence)\n    That is: a_j = i\n    with e.g. max_jump = 100\n    from[-max_jump, max_jump] to [0, 2*max_jump + 1] \n    \"\"\"\n    # We normalise j by the lenght of the French sentence and scale the result to the length of the English sentence\n    # this gives us a continuous value that is an interpolation of where we j would be in the English sentence\n    # if alignments were a linear function of the length ratio\n    jump = np.floor(i - (j * m / n)) \n    # then we collapse all jumps that are too far to the right to the maximum jump value allowed\n    if jump > max_jump:  # or we collapse all jumps that are too far to the left to the maximum (negative) jump allowed\n        jump = max_jump   \n    elif jump < -max_jump:\n        jump = -max_jump\n    # Now we shift the jump values so they start from 0\n    #  this is only necessary if you use python lists or numpy vectors to store jump probabilities\n    #  otherwise, you can use a python dict and this shifting is not required since dicts can have negative keys\n    idx = (int)(jump + max_jump)\n    if idx >= 2*max_jump: # Fix for out of bounds index\n        idx -= 1\n    return idx","metadata":{},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def create_alignments_modelII(t, jump_dist, max_jump, file_f, file_e, target, file_enc='utf-8', null='<NULL>'):\n    # open file to write to\n    with open(target,'w',encoding=file_enc) as tar:\n        # for each sentence in list\n        for line_num, (f_sentence,e_sentence) in tqdm(enumerate(zip(file_f,file_e)), total=len(file_f), desc='AlignII', leave=True):\n            # Get lengths\n            l = len(e_sentence)\n            m = len(f_sentence)\n            f_sentence = [None] + f_sentence\n            e_sentence = [null] + e_sentence\n\n            # for each word position in sentence, find the best alignment\n            for i in range(1, m+1): # french\n                max_p = 0\n                max_ind = 0 #when no alignment is found, align to zero\n                f = f_sentence[i]\n                for j in range(0, l+1): # english\n                    e = e_sentence[j]\n\n                    if (f,e) in t:\n                        p = t[(f,e)]*jump_dist[0, jump_func(j,i,l,m,max_jump)]\n\n                        if p >= max_p:\n                            max_p = p\n                            max_ind = j\n\n                if max_ind != 0: # Skip null alignments\n                    # write to file. Output: sentence_line english_pos french_pos probability\n                    tar.write('%d %d %d P %f\\n'%(line_num, max_ind, i, max_p)) ","metadata":{},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def calculate_log_likelihood_modelII(t, file_f, file_e, jump_dist, max_jump, null='<NULL>'):\n    log_likelihood = 0\n    N = 0\n    for sentence_f, sentence_e in tqdm(zip(file_f,file_e),total=len(file_f),  desc='Calc LL', leave=True):\n        l = len(sentence_e)\n        m = len(sentence_f)\n\n        sentence_e = [null] + sentence_e\n        sentence_f = [None] + sentence_f\n        \n        for i in range(1, m+1): # french\n            f = sentence_f[i]\n            max_p = 0\n            max_align_p = 0\n            for j in range(0, l+1): # english\n                e = sentence_e[j]\n                if (f,e) in t and t[(f,e)] >= max_p:\n                    max_p = t[(f,e)]\n                    max_align_p = jump_dist[0, jump_func(j,i,l,m,max_jump)]                \n            N += 1\n            log_likelihood += np.log(max_align_p) + np.log(max_p)\n    return log_likelihood, N","metadata":{},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Shared","metadata":{}},{"cell_type":"code","source":"def em_algorithm(model,\n                 t=None, #Only used for model II\n                 max_epoch=10, \n                 threshold=0.01,\n                 initial_method=Initialization_type.uniform, #How to initialize t\n                 terminate_method=Termination_type.epochs, \n                 train_file_f='data/training/hansards.36.2.f',\n                 train_file_e='data/training/hansards.36.2.e',\n                 validation_file_f='data/validation/dev.f',\n                 validation_file_e='data/validation/dev.e',\n                 validation_truth='data/validation/dev.wa.nonullalign',\n                 test_file_f = 'data/testing/test/test.f',\n                 test_file_e = 'data/testing/test/test.e',\n                 test_truth = 'data/testing/answers/test.wa.nonullalign',\n                 pickles_path='data/pickles/',\n                 align_path='data/alignments/',\n                 save_prefix='',\n                 save_pickles=True,\n                 use_VB=False,\n                 calc_LL_train=True,\n                 alpha=0.1, #Only used if VB is used\n                 file_enc='utf-8'):\n    \n    # test if prefix exists and correct format\n    if save_prefix != '' and save_prefix[-1]!='_':\n        save_prefix+='_'\n    \n    # read in all the files\n    with open(train_file_f, encoding=file_enc) as f:\n        train_file_f = f.readlines()\n    with open(train_file_e, encoding=file_enc) as f:\n        train_file_e = f.readlines()\n    with open(validation_file_f, encoding=file_enc) as f:\n        validation_file_f = f.readlines()\n    with open(validation_file_e, encoding=file_enc) as f:\n        validation_file_e = f.readlines()\n    with open(test_file_f, encoding=file_enc) as f:\n        test_file_f = f.readlines()\n    with open(test_file_e, encoding=file_enc) as f:\n        test_file_e = f.readlines()\n    \n    # get word pairs from corpus\n    pairs, vocab_f, vocab_e = create_pairs_and_update_files(train_file_f, train_file_e)\n    update_files(vocab_f, vocab_e, validation_file_f, validation_file_e)\n    update_files(vocab_f, vocab_e, test_file_f, test_file_e)\n\n    #initialize parameters\n    if model == IBM_model.I:\n        t = init_params_modelI(initial_method, pairs)\n    elif model == IBM_model.II:\n        # For jump function\n        max_jump = 100        \n        t, jump_dist = init_params_modelII(initial_method, pairs, max_jump, t)\n    \n    tracker = Metrics_tracker(save_prefix, \n                              align_path, \n                              validation_truth, \n                              validation_file_f, \n                              validation_file_e,\n                              train_file_f,\n                              train_file_e,\n                              test_truth,\n                              test_file_f,\n                              test_file_e,\n                              vocab_f,\n                              vocab_e,\n                              calc_LL_train,\n                              file_enc)\n    \n    # calculate initial scores before training\n    tracker.track_metrics(0, model, t, jump_dist if model == IBM_model.II else None, \n                                       max_jump if model == IBM_model.II else None)\n    \n    \n    #print train result\n    tracker.print_last_metrics('Init')\n        \n    # loop for max_epochs or till convergence is reached\n    for epoch in range(1,max_epoch+1):\n        print(\"start epoch: \"+str(epoch))\n        \n        # do an EM step\n        if model == IBM_model.I:\n            t = em_step_modelI(t, train_file_f, train_file_e, use_VB, alpha)\n        else:\n            t, jump_dist = em_step_modelII(t, jump_dist, max_jump, train_file_f, train_file_e)\n        \n        # create AER results and calculate the loglikelihoods/perplexity\n        tracker.track_metrics(epoch, model, t, jump_dist if model == IBM_model.II else None, \n                                               max_jump if model == IBM_model.II else None)\n        tracker.print_last_metrics(epoch)\n        \n        #store train progress\n        if save_pickles:\n            pickle.dump(t, open( os.path.join(pickles_path,'{0}t_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n            if model == IBM_model.II:\n                pickle.dump(jump_dist, open( os.path.join(pickles_path,'{0}jump_dist_epoch{1}.p'.format(save_prefix,epoch)), \"wb\" ))\n        \n        #test for convergence\n        if terminate_method == Termination_type.perplexity_convergence:\n            if (len(tracker.train_perplexities) > 2) and (abs(tracker.train_perplexities[-2]-train_perplexity) < threshold):\n                print('Reached Convergence!')\n                break\n    \n    # Dump metrics to pickles\n    test_aer = tracker.calculate_aer_test(epoch+1,\n                                   model,\n                                   t,\n                                   jump_dist if model == IBM_model.II else None,\n                                   max_jump if model == IBM_model.II else None)\n    print('=================\\nTEST AER RESULT: {0}\\n================='.format(test_aer))\n    tracker.save_metrics(os.path.join(pickles_path,'{}metrics.p'.format(save_prefix)))\n    \n    if model == IBM_model.I:\n        return t\n    elif model == IBM_model.II:\n        return t, jump_dist","metadata":{},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"---\n# RUNNING THE SCRIPT","metadata":{}},{"cell_type":"markdown","source":"### RUNS\n---","metadata":{}},{"cell_type":"code","source":"# Run model I\nt = em_algorithm(model=IBM_model.I, max_epoch=10, save_prefix='modelI_report')\n#                  ,\n#                  train_file_f='data/validation/dev.f',\n#                  train_file_e='data/validation/dev.e')","metadata":{},"execution_count":47,"outputs":[{"text":"Count Words:   0%|          | 195/231164 [00:00<01:58, 1949.17it/s]","name":"stderr","output_type":"stream"},{"text":"creating pairs\n","name":"stdout","output_type":"stream"},{"text":"Count Words: 100%|██████████| 231164/231164 [01:56<00:00, 1976.63it/s]\nPairs: 100%|██████████| 231164/231164 [01:36<00:00, 2386.56it/s]\nInit Norm: 100%|██████████| 11614796/11614796 [00:14<00:00, 810577.80it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1739.05it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:56<00:00, 1307.98it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1675.46it/s]\nE-step:   0%|          | 141/231164 [00:00<05:29, 700.52it/s]","name":"stderr","output_type":"stream"},{"text":"Results Epoch: Init\n====================\nAER:\n\t validation:\t0.9065155807365439\nLog Likelihood:\n\t train:\t\t-2684.922037257008\n\t validation:\t-1613.9292653144628\nPerplexity:\n\t train:\t\t1.0005829946141471\n\t validation:\t9.378871556029178\nstart epoch: 1\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:30<00:00, 513.39it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:32<00:00, 354430.33it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1683.11it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:43<00:00, 1411.99it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1610.82it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 1\n====================\nAER:\n\t validation:\t0.8016997167138811\nLog Likelihood:\n\t train:\t\t-2415.721303235616\n\t validation:\t-1352.3986734140342\nPerplexity:\n\t train:\t\t1.0005245259776225\n\t validation:\t6.525557012167654\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 86/231164 [00:00<04:31, 851.74it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 2\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:50<00:00, 491.22it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:29<00:00, 396321.61it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1731.69it/s]\nCalc LL: 100%|██████████| 231164/231164 [20:05<00:00, 191.77it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1675.46it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 2\n====================\nAER:\n\t validation:\t0.49008498583569404\nLog Likelihood:\n\t train:\t\t-2401.468665188581\n\t validation:\t-1339.9927730119987\nPerplexity:\n\t train:\t\t1.0005214304932093\n\t validation:\t6.414235365164364\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 133/231164 [00:00<05:53, 654.00it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 3\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:37<00:00, 504.86it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:31<00:00, 368805.96it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1600.53it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:40<00:00, 1443.76it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1826.16it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 3\n====================\nAER:\n\t validation:\t0.4230406043437205\nLog Likelihood:\n\t train:\t\t-2395.263123189799\n\t validation:\t-1334.9009834654034\nPerplexity:\n\t train:\t\t1.0005200827345826\n\t validation:\t6.36909683068159\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 81/231164 [00:00<04:48, 801.57it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 4\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:26<00:00, 518.16it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:28<00:00, 413393.11it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1729.63it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:36<00:00, 1477.73it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1746.96it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 4\n====================\nAER:\n\t validation:\t0.39565627950897075\nLog Likelihood:\n\t train:\t\t-2392.868572885601\n\t validation:\t-1333.2165482318226\nPerplexity:\n\t train:\t\t1.000519562671578\n\t validation:\t6.354234405110042\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 86/231164 [00:00<04:29, 858.29it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 5\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:28<00:00, 514.85it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:29<00:00, 391693.81it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1627.95it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:36<00:00, 1475.12it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1743.56it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 5\n====================\nAER:\n\t validation:\t0.38243626062322944\nLog Likelihood:\n\t train:\t\t-2391.7396050427124\n\t validation:\t-1332.6361177644642\nPerplexity:\n\t train:\t\t1.0005193174755664\n\t validation:\t6.349121080447569\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 85/231164 [00:00<04:33, 843.69it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 6\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:26<00:00, 517.77it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:35<00:00, 325941.80it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1651.78it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:36<00:00, 1477.36it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1748.69it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 6\n====================\nAER:\n\t validation:\t0.3729933899905571\nLog Likelihood:\n\t train:\t\t-2391.112790368089\n\t validation:\t-1332.4648036622189\nPerplexity:\n\t train:\t\t1.0005191813402197\n\t validation:\t6.347612668843216\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 87/231164 [00:00<04:29, 858.48it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 7\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:26<00:00, 517.33it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:28<00:00, 409809.38it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1721.84it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:36<00:00, 1473.90it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1668.72it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 7\n====================\nAER:\n\t validation:\t0.3682719546742209\nLog Likelihood:\n\t train:\t\t-2390.795344446599\n\t validation:\t-1332.551271484064\nPerplexity:\n\t train:\t\t1.0005191123954296\n\t validation:\t6.348373968641693\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 83/231164 [00:00<04:40, 822.94it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 8\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:31<00:00, 512.38it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:30<00:00, 374216.98it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1494.00it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:36<00:00, 1475.45it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1046.92it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 8\n====================\nAER:\n\t validation:\t0.36449480642115206\nLog Likelihood:\n\t train:\t\t-2390.6379785837767\n\t validation:\t-1332.7894490089827\nPerplexity:\n\t train:\t\t1.0005190782177789\n\t validation:\t6.350471457924301\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 83/231164 [00:00<04:41, 819.91it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 9\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:34<00:00, 508.79it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:29<00:00, 390204.71it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1418.61it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:43<00:00, 1414.82it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1581.48it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 9\n====================\nAER:\n\t validation:\t0.3616619452313503\nLog Likelihood:\n\t train:\t\t-2390.5883109717\n\t validation:\t-1333.0700752772539\nPerplexity:\n\t train:\t\t1.0005190674306728\n\t validation:\t6.35294365760056\n","name":"stdout","output_type":"stream"},{"text":"E-step:   0%|          | 88/231164 [00:00<04:30, 854.13it/s]","name":"stderr","output_type":"stream"},{"text":"start epoch: 10\n","name":"stdout","output_type":"stream"},{"text":"E-step: 100%|██████████| 231164/231164 [07:35<00:00, 507.32it/s]\nM-Step: 100%|██████████| 11583295/11583295 [00:32<00:00, 360098.15it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 1731.21it/s]\nCalc LL: 100%|██████████| 231164/231164 [02:34<00:00, 1499.75it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 1781.55it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 10\n====================\nAER:\n\t validation:\t0.3616619452313503\nLog Likelihood:\n\t train:\t\t-2390.6066612801187\n\t validation:\t-1333.335899558221\nPerplexity:\n\t train:\t\t1.0005190714161012\n\t validation:\t6.355286345578848\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 447/447 [00:00<00:00, 2464.37it/s]\n","name":"stderr","output_type":"stream"},{"text":"=================\nTEST AER RESULT: 0.352063734214764\n=================\n","name":"stdout","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Diana Runs","metadata":{}},{"cell_type":"code","source":"t = em_algorithm(model=IBM_model.II,initial_method=Initialization_type.random, max_epoch=5, calc_LL_train=False, save_prefix='modelII_init_random3',\n                train_file_f='NLP2/NLP2-Projects/Project1/data/training/hansards.36.2.f',\ntrain_file_e='NLP2/NLP2-Projects/Project1/data/training/hansards.36.2.e',\nvalidation_file_f='NLP2/NLP2-Projects/Project1/data/validation/dev.f',\nvalidation_file_e='NLP2/NLP2-Projects/Project1/data/validation/dev.e',\nvalidation_truth='NLP2/NLP2-Projects/Project1/data/validation/dev.wa.nonullalign',\ntest_file_f = 'NLP2/NLP2-Projects/Project1/data/testing/test/test.f',\ntest_file_e = 'NLP2/NLP2-Projects/Project1/data/testing/test/test.e',\ntest_truth = 'NLP2/NLP2-Projects/Project1/data/testing/answers/test.wa.nonullalign',\npickles_path='NLP2/NLP2-Projects/Project1/data/pickles/',\nalign_path='NLP2/NLP2-Projects/Project1/data/alignments/')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","text":"creating pairs\n","output_type":"stream"},{"name":"stderr","text":"Count Words: 100%|████████████████████| 231164/231164 [04:33<00:00, 846.04it/s]\nPairs: 100%|██████████████████████████| 231164/231164 [04:43<00:00, 814.17it/s]\nAlignII: 100%|█████████████████████████████████| 37/37 [00:00<00:00, 45.48it/s]\nCalc LL: 100%|████████████████████████████████| 37/37 [00:00<00:00, 171.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Results Epoch: Init\n====================\nAER:\n\t validation:\t0.896551724137931\nLog Likelihood:\n\t train:\t\t-1\n\t validation:\t-3866.965145717926\nPerplexity:\n\t train:\t\t-1\n\t validation:\t213.43574923865765\nstart epoch: 1\n","output_type":"stream"},{"name":"stderr","text":"E-step:  93%|██████████████████████▎ | 215201/231164 [1:51:38<08:16, 32.13it/s]","output_type":"stream"}]},{"cell_type":"markdown","source":"# Trained  models evaluation","metadata":{}},{"cell_type":"code","source":"file_path = 'data/pickles/modelI_report_t_epoch'\nfile_path_jump = 'data/pickles/modelII_report_ibm1_jump_dist_epoch'\nsave_prefix='modelI_just_evaluation_'\nmax_epochs = 10\nmax_jump = 100\nmodel = IBM_model.I\ninitial_method=Initialization_type.uniform\nfile_enc='utf-8'\nvalidation_file_f='data/validation/dev.f' \nvalidation_file_e='data/validation/dev.e'\ntrain_file_f='data/training/hansards.36.2.f'\ntrain_file_e='data/training/hansards.36.2.e'\ntest_file_f = 'data/testing/test/test.f'\ntest_file_e = 'data/testing/test/test.e'\n\nwith open(train_file_f, encoding=file_enc) as f:\n    train_file_f = f.readlines()\nwith open(train_file_e, encoding=file_enc) as f:\n    train_file_e = f.readlines()\nwith open(validation_file_f, encoding=file_enc) as f:\n    validation_file_f = f.readlines()\nwith open(validation_file_e, encoding=file_enc) as f:\n    validation_file_e = f.readlines()\nwith open(test_file_f, encoding=file_enc) as f:\n    test_file_f = f.readlines()\nwith open(test_file_e, encoding=file_enc) as f:\n    test_file_e = f.readlines()","metadata":{},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"pairs, vocab_f, vocab_e = create_pairs_and_update_files(train_file_f, train_file_e)\nupdate_files(vocab_f, vocab_e, validation_file_f, validation_file_e)\nupdate_files(vocab_f, vocab_e, test_file_f, test_file_e) ","metadata":{},"execution_count":51,"outputs":[{"text":"\nCount Words:   0%|          | 0/231164 [00:00<?, ?it/s]\u001b[A\nCount Words:   0%|          | 390/231164 [00:00<01:00, 3845.57it/s]\u001b[A\n","name":"stderr","output_type":"stream"},{"text":"creating pairs\n","name":"stdout","output_type":"stream"},{"text":"Count Words:   0%|          | 703/231164 [00:00<01:05, 3493.21it/s]\u001b[A\nCount Words:   1%|          | 1276/231164 [00:00<00:54, 4212.61it/s]\u001b[A\nCount Words:   1%|          | 1664/231164 [00:00<00:55, 4133.72it/s]\u001b[A\nCount Words:   1%|          | 1986/231164 [00:00<01:03, 3635.10it/s]\u001b[A\nCount Words:   1%|          | 2278/231164 [00:00<01:11, 3217.93it/s]\u001b[A\nCount Words:   1%|          | 2538/231164 [00:00<01:12, 3139.46it/s]\u001b[A\nCount Words:   1%|          | 2871/231164 [00:00<01:12, 3160.35it/s]\u001b[A\nCount Words:   1%|▏         | 3208/231164 [00:01<01:11, 3168.56it/s]\u001b[A\nCount Words:   2%|▏         | 3506/231164 [00:01<01:12, 3134.72it/s]\u001b[A\nCount Words:   2%|▏         | 3888/231164 [00:01<01:11, 3183.28it/s]\u001b[A\nCount Words:   2%|▏         | 4219/231164 [00:01<01:11, 3192.83it/s]\u001b[A\nCount Words:   2%|▏         | 4539/231164 [00:01<01:11, 3169.29it/s]\u001b[A\nCount Words:   2%|▏         | 5175/231164 [00:01<01:06, 3377.35it/s]\u001b[A\nCount Words:   2%|▏         | 5583/231164 [00:01<01:07, 3349.03it/s]\u001b[A\nCount Words:   3%|▎         | 5960/231164 [00:01<01:07, 3360.50it/s]\u001b[A\nCount Words:   3%|▎         | 6330/231164 [00:01<01:07, 3348.22it/s]\u001b[A\nCount Words:   3%|▎         | 6684/231164 [00:01<01:06, 3354.49it/s]\u001b[A\nCount Words:   3%|▎         | 7124/231164 [00:02<01:05, 3405.55it/s]\u001b[A\nCount Words:   3%|▎         | 7547/231164 [00:02<01:04, 3442.97it/s]\u001b[A\nCount Words:   3%|▎         | 7939/231164 [00:02<01:04, 3440.86it/s]\u001b[A\nCount Words:   4%|▎         | 8316/231164 [00:02<01:04, 3428.98it/s]\u001b[A\nCount Words:   4%|▍         | 8785/231164 [00:02<01:03, 3482.25it/s]\u001b[A\nCount Words:   4%|▍         | 9179/231164 [00:02<01:03, 3488.49it/s]\u001b[A\nCount Words:   4%|▍         | 9627/231164 [00:02<01:02, 3522.82it/s]\u001b[A\nCount Words:   4%|▍         | 10029/231164 [00:02<01:03, 3509.50it/s]\u001b[A\nCount Words:   5%|▍         | 10407/231164 [00:02<01:03, 3499.86it/s]\u001b[A\nCount Words:   5%|▍         | 11067/231164 [00:03<01:01, 3596.65it/s]\u001b[A\nCount Words:   5%|▍         | 11518/231164 [00:03<01:00, 3607.39it/s]\u001b[A\nCount Words:   5%|▌         | 11948/231164 [00:03<01:00, 3613.68it/s]\u001b[A\nCount Words:   5%|▌         | 12373/231164 [00:03<01:00, 3633.03it/s]\u001b[A\nCount Words:   6%|▌         | 12791/231164 [00:03<01:00, 3635.39it/s]\u001b[A\nCount Words:   6%|▌         | 13195/231164 [00:03<01:00, 3615.95it/s]\u001b[A\nCount Words:   6%|▌         | 13907/231164 [00:03<00:58, 3709.48it/s]\u001b[A\nCount Words:   6%|▌         | 14384/231164 [00:03<00:58, 3677.49it/s]\u001b[A\nCount Words:   6%|▋         | 14806/231164 [00:04<00:59, 3663.73it/s]\u001b[A\nCount Words:   7%|▋         | 15364/231164 [00:04<00:58, 3709.76it/s]\u001b[A\nCount Words:   7%|▋         | 15806/231164 [00:04<00:57, 3725.83it/s]\u001b[A\nCount Words:   7%|▋         | 16248/231164 [00:04<00:58, 3698.99it/s]\u001b[A\nCount Words:   7%|▋         | 16645/231164 [00:04<00:58, 3657.22it/s]\u001b[A\nCount Words:   7%|▋         | 16998/231164 [00:04<00:58, 3635.89it/s]\u001b[A\nCount Words:   8%|▊         | 17704/231164 [00:04<00:57, 3707.33it/s]\u001b[A\nCount Words:   8%|▊         | 18148/231164 [00:04<00:57, 3708.98it/s]\u001b[A\nCount Words:   8%|▊         | 18572/231164 [00:05<00:57, 3685.58it/s]\u001b[A\nCount Words:   8%|▊         | 18956/231164 [00:05<00:58, 3654.52it/s]\u001b[A\nCount Words:   9%|▊         | 19675/231164 [00:05<00:56, 3721.35it/s]\u001b[A\nCount Words:   9%|▉         | 20233/231164 [00:05<00:56, 3755.70it/s]\u001b[A\nCount Words:   9%|▉         | 20721/231164 [00:05<00:56, 3734.76it/s]\u001b[A\nCount Words:   9%|▉         | 21154/231164 [00:05<00:56, 3731.56it/s]\u001b[A\nCount Words:   9%|▉         | 21578/231164 [00:05<00:56, 3740.07it/s]\u001b[A\nCount Words:  10%|▉         | 21992/231164 [00:05<00:56, 3734.66it/s]\u001b[A\nCount Words:  10%|▉         | 22386/231164 [00:06<00:56, 3727.30it/s]\u001b[A\nCount Words:  10%|▉         | 22763/231164 [00:06<00:55, 3727.25it/s]\u001b[A\nCount Words:  10%|█         | 23209/231164 [00:06<00:55, 3738.24it/s]\u001b[A\nCount Words:  10%|█         | 23605/231164 [00:06<00:55, 3724.46it/s]\u001b[A\nCount Words:  10%|█         | 24012/231164 [00:06<00:55, 3729.69it/s]\u001b[A\nCount Words:  11%|█         | 24392/231164 [00:06<00:55, 3722.15it/s]\u001b[A\nCount Words:  11%|█         | 24757/231164 [00:06<00:55, 3690.89it/s]\u001b[A\nCount Words:  11%|█         | 25258/231164 [00:06<00:55, 3710.15it/s]\u001b[A\nCount Words:  11%|█         | 25636/231164 [00:06<00:55, 3709.94it/s]\u001b[A\nCount Words:  11%|█▏        | 26012/231164 [00:07<00:55, 3708.59it/s]\u001b[A\nCount Words:  11%|█▏        | 26407/231164 [00:07<00:55, 3711.73it/s]\u001b[A\nCount Words:  12%|█▏        | 26862/231164 [00:07<00:54, 3723.07it/s]\u001b[A\nCount Words:  12%|█▏        | 27262/231164 [00:07<00:54, 3718.95it/s]\u001b[A\nCount Words:  12%|█▏        | 27646/231164 [00:07<00:54, 3716.30it/s]\u001b[A\nCount Words:  12%|█▏        | 28097/231164 [00:07<00:54, 3726.96it/s]\u001b[A\nCount Words:  12%|█▏        | 28755/231164 [00:07<00:53, 3764.15it/s]\u001b[A\nCount Words:  13%|█▎        | 29231/231164 [00:07<00:53, 3753.91it/s]\u001b[A\nCalc LL:  10%|█         | 23929/231164 [00:30<04:20, 795.74it/s] \nCount Words: 100%|██████████| 231164/231164 [01:10<00:00, 3273.24it/s]\nPairs: 100%|██████████| 231164/231164 [01:04<00:00, 3603.14it/s]\n","name":"stderr","output_type":"stream"}]},{"cell_type":"code","source":"# ttemp = pickle.load(open( \"data/pickles/modelI_report_t_epoch10.p\", \"rb\" ) )\n# ttemp, jump_dist = init_params_modelI(initial_method, pairs, max_jump, ttemp)\nttemp = init_params_modelI(initial_method, pairs)\n\ntracker = Metrics_tracker(save_prefix=save_prefix,\n                          align_path='data/alignments/', \n                          validation_truth='data/validation/dev.wa.nonullalign', \n                          validation_file_f=validation_file_f, \n                          validation_file_e=validation_file_e,\n                          train_file_f=train_file_f,\n                          train_file_e=train_file_e,\n                          test_truth = 'data/testing/answers/test.wa.nonullalign',\n                          test_file_f=test_file_f,\n                          test_file_e=test_file_e,\n                          vocab_f=vocab_f,\n                          vocab_e=vocab_e,\n                          calc_LL_train=True,\n                          file_enc='utf-8')\ntracker.track_metrics(0, model, ttemp, jump_dist if model == IBM_model.II else None, \n                                       max_jump if model == IBM_model.II else None)\ntracker.print_last_metrics('init')\nfor epoch in range(1,max_epochs+1):\n    path = file_path+str(epoch)+str('.p')\n    if model == IBM_model.II:\n        path_jump = file_path_jump+str(epoch)+str('.p')\n        jump_dist = pickle.load(open( path_jump, \"rb\" ) )\n    ttemp = pickle.load(open( path, \"rb\" ) )\n    tracker.track_metrics(0, model, ttemp, jump_dist if model == IBM_model.II else None, \n                                           max_jump if model == IBM_model.II else None)\n    tracker.print_last_metrics(epoch)","metadata":{},"execution_count":53,"outputs":[{"text":"Init Norm: 100%|██████████| 11614796/11614796 [00:08<00:00, 1358851.04it/s]\nAlignI: 100%|██████████| 37/37 [00:00<00:00, 2189.99it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:21<00:00, 2834.70it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 3091.36it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: init\n====================\nAER:\n\t validation:\t0.9065155807365439\nLog Likelihood:\n\t train:\t\t-95356501.9183324\n\t validation:\t-14695.750330280907\nPerplexity:\n\t train:\t\t976363281.1010088\n\t validation:\t711193698.5401267\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2647.24it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:37<00:00, 2380.25it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2196.19it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 1\n====================\nAER:\n\t validation:\t0.8016997167138811\nLog Likelihood:\n\t train:\t\t-33126983.437462915\n\t validation:\t-5019.118429964895\nPerplexity:\n\t train:\t\t1327.4057958289181\n\t validation:\t1055.0350904742213\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2689.54it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:40<00:00, 2306.80it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2696.36it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 2\n====================\nAER:\n\t validation:\t0.49008498583569404\nLog Likelihood:\n\t train:\t\t-29832286.615824815\n\t validation:\t-4560.100115089553\nPerplexity:\n\t train:\t\t649.2327344497609\n\t validation:\t558.1836851400658\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2544.46it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:35<00:00, 2418.12it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2818.19it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 3\n====================\nAER:\n\t validation:\t0.4230406043437205\nLog Likelihood:\n\t train:\t\t-28397788.705085233\n\t validation:\t-4371.703901865533\nPerplexity:\n\t train:\t\t475.51581487640476\n\t validation:\t429.82992206703176\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2577.08it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:41<00:00, 2275.10it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2481.00it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 4\n====================\nAER:\n\t validation:\t0.39565627950897075\nLog Likelihood:\n\t train:\t\t-27844254.87926536\n\t validation:\t-4309.379798223086\nPerplexity:\n\t train:\t\t421.678323675536\n\t validation:\t394.23548267551024\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2755.05it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:34<00:00, 2442.79it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2641.02it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 5\n====================\nAER:\n\t validation:\t0.38243626062322944\nLog Likelihood:\n\t train:\t\t-27583278.15624213\n\t validation:\t-4287.903870930768\nPerplexity:\n\t train:\t\t398.45385432505185\n\t validation:\t382.6658276299712\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2546.55it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:46<00:00, 2179.66it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2424.30it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 6\n====================\nAER:\n\t validation:\t0.3729933899905571\nLog Likelihood:\n\t train:\t\t-27438381.168903857\n\t validation:\t-4281.565249147855\nPerplexity:\n\t train:\t\t386.1162283604535\n\t validation:\t379.3163921318975\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2752.02it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:37<00:00, 2364.31it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2673.37it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 7\n====================\nAER:\n\t validation:\t0.3682719546742209\nLog Likelihood:\n\t train:\t\t-27364999.099623423\n\t validation:\t-4284.7645585559085\nPerplexity:\n\t train:\t\t380.01440123519365\n\t validation:\t381.003281100288\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2453.90it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:44<00:00, 2213.31it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2420.78it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 8\n====================\nAER:\n\t validation:\t0.36449480642115206\nLog Likelihood:\n\t train:\t\t-27328621.777428117\n\t validation:\t-4293.577126977958\nPerplexity:\n\t train:\t\t377.02541788527014\n\t validation:\t385.6887470101314\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2501.00it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:39<00:00, 2321.45it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2253.27it/s]\n","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 9\n====================\nAER:\n\t validation:\t0.3616619452313503\nLog Likelihood:\n\t train:\t\t-27317140.41389409\n\t validation:\t-4303.960298904077\nPerplexity:\n\t train:\t\t376.0869286112625\n\t validation:\t391.2832647331653\n","name":"stdout","output_type":"stream"},{"text":"AlignI: 100%|██████████| 37/37 [00:00<00:00, 2581.15it/s]\nCalc LL: 100%|██████████| 231164/231164 [01:46<00:00, 2162.27it/s]\nCalc LL: 100%|██████████| 37/37 [00:00<00:00, 2248.89it/s]","name":"stderr","output_type":"stream"},{"text":"Results Epoch: 10\n====================\nAER:\n\t validation:\t0.3616619452313503\nLog Likelihood:\n\t train:\t\t-27321382.344021343\n\t validation:\t-4313.795797299753\nPerplexity:\n\t train:\t\t376.43339253109247\n\t validation:\t396.65751584680845\n","name":"stdout","output_type":"stream"},{"text":"\n","name":"stderr","output_type":"stream"}]},{"cell_type":"code","source":"tracker.save_metrics('data/pickles/modelII_evaluation_metrics_ibm1.p')","metadata":{},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}