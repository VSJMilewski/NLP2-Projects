{"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":2,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project 2","metadata":{}},{"cell_type":"markdown","source":"##### import needed packages","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm_notebook, tqdm, tnrange\nfrom collections import defaultdict,Counter\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nnp.set_printoptions(threshold=np.nan)","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Global for enabling GPU","metadata":{}},{"cell_type":"code","source":"run_gpu = torch.cuda.is_available()\n# run_gpu = False\nrun_gpu","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Classes and Functions","metadata":{}},{"cell_type":"markdown","source":"## Data Manipulation","metadata":{}},{"cell_type":"markdown","source":"### Cleaning and preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess(train_data, val_data, test_data, vocab_size=10000):\n    \n    # loop over all the given files\n    for data in [train_data, val_data, test_data]:\n        # contains a source and a target file\n        for k,v in data.items():\n            tokenized_path = v[:v.find('.')] + '_tokenized.{}'.format(k)\n\n            # Tokenize \n            tokenize_command = 'perl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l {lang} < {file_path} > {output_path}'.format(\n                lang=k, file_path=v, output_path=tokenized_path)\n            print('tokenize command:\\t{}'.format(tokenize_command))\n            \n            # Lowercase\n            lowercase_path = tokenized_path[:tokenized_path.find('.')] + '_lowercased.{}'.format(k)\n            lowercase_command = 'perl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < {file_path} > {output_path}'.format(\n                file_path=tokenized_path, output_path=lowercase_path)\n            print('lowercase command:\\t{}\\n'.format(lowercase_command))\n            \n    # BPE\n    # Get vocabulary using train data\n    script_name = 'python tools/subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py'\n    args = ' --input {train_en} {train_fr} -s {num_symbols} -o {codes_file} --write-vocabulary {vocab_file}.en {vocab_file}.fr'\n    substr_index = train_data['en'].find('/')\n    vocab_file_name = train_data['en'][:substr_index] + '/vocab'\n    codes_file_name = train_data['en'][:substr_index] + '/codes.bpe'\n    learn_vocab_command = script_name + args.format(\n        train_en='data/train/train_tokenized_lowercased.en',\n        train_fr='data/train/train_tokenized_lowercased.fr',\n        num_symbols=str(vocab_size),\n        codes_file=codes_file_name,\n        vocab_file=vocab_file_name\n    )\n    print('learn vocab command:\\t{}'.format(learn_vocab_command))\n    \n    # Process all files the same way for consistency\n    script_name = 'python tools/subword-nmt/subword_nmt/apply_bpe.py'\n    for data in [train_data, val_data, test_data]:\n        for k,v in data.items():\n            args = ' -c {codes_file} --vocabulary {vocab_file}.{lang} --vocabulary-threshold 50 < {train_file}.{lang} > {train_file}_bpe.{lang}'\n            train_file_name = v[:v.find('.')] + '_tokenized_lowercased'\n            bpe_command = script_name + args.format(\n                codes_file=codes_file_name,\n                vocab_file=vocab_file_name,\n                lang=k,\n                train_file=train_file_name\n            )\n            print('bpe command:\\t{}'.format(bpe_command))","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Building dictionaries and vocabularies","metadata":{}},{"cell_type":"code","source":"PAD = '<PAD>'\nUNK = '<UNK>'\nSTART = '<SOS>'\nEND = '<EOS>'","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DataProcessor():\n    def __init__(self, file_name, vocab_size):\n        self.max_sentence_length = -1\n        self.vocab_size = vocab_size\n        \n        self.file_name = file_name\n        self.sentences = self.load_data()\n        self.vocab,self.vocab_size = self.build_vocab()\n        self.w2i, self.i2w = self.build_dicts()        \n        \n    def load_data(self):\n        sentences = []\n        with open(self.file_name, 'r') as f:\n            for line in f:\n                sentences.append(line.split())                \n        return sentences\n    \n    def build_dicts(self):\n        \"\"\"\n        creates lookup tables to find the index given the word \n        and the otherway around \n        \"\"\"\n        w2i = defaultdict(lambda: w2i[UNK])# would something like this work? not entirely, needs some tweaking\n        i2w = dict()\n        for i,w in enumerate(self.vocab):\n            i2w[i] = w\n            w2i[w] = i\n        return w2i, i2w    \n    \n    def build_vocab(self): \n        \"\"\"\n        builds a vocabulary with the most occuring words, in addition to\n        the UNK token at index 1 and PAD token at index 0. \n        START and END tokens are added to the vocabulary through the\n        preprocessed sentences.\n        with vocab size none, all existing words in the data are used\n        \"\"\"\n        vocab = Counter()\n        for s in self.sentences:\n            l = len(s)\n            if l > self.max_sentence_length:\n                self.max_sentence_length = l\n            for w in s:\n                vocab[w] += 1\n\n        vocab = [k for k,_ in vocab.most_common(self.vocab_size)]\n        vocab = [PAD,UNK,START,END] + vocab\n        return vocab,len(vocab)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Getting data batches","metadata":{}},{"cell_type":"code","source":"def batch_generator(source_processor, target_processor, batch_size):\n    idx = np.arange(len(source_processor.sentences))\n    \n    if batch_size == 1:\n        bi = 0\n        while True:\n            b_words_source = np.zeros([batch_size, source_processor.max_sentence_length+1])\n            b_positions_source = np.zeros([batch_size, source_processor.max_sentence_length+1])\n            b_words_target = np.zeros([batch_size, target_processor.max_sentence_length+2])\n            \n            sent_source = source_processor.sentences[bi] + [END]\n            sent_target = [START] + target_processor.sentences[bi] + [END]\n                \n            b_words_source[0, :len(sent_source)] = np.array([source_processor.w2i[w] for w in sent_source])\n            b_positions_source[0, :len(sent_source)] = np.array([i for i in range(len(sent_source))])\n            b_words_target[0, :len(sent_target)] = np.array([target_processor.w2i[w] for w in sent_target])\n            \n            if run_gpu:\n                word_tens = torch.from_numpy(b_words_source).type(torch.cuda.LongTensor)\n                pos_tens = torch.from_numpy(b_positions_source).type(torch.cuda.LongTensor)\n                tar_tens = torch.from_numpy(b_words_target).type(torch.cuda.LongTensor)\n                sentence_lengths_source = torch.cuda.FloatTensor([len(sent_source)])\n                sentence_lengths_target = torch.cuda.FloatTensor([len(sent_target)])\n            else:\n                word_tens = torch.from_numpy(b_words_source).type(torch.LongTensor)\n                pos_tens = torch.from_numpy(b_positions_source).type(torch.LongTensor)\n                tar_tens = torch.from_numpy(b_words_target).type(torch.LongTensor)\n                sentence_lengths_source = torch.FloatTensor([len(sent_source)])\n                sentence_lengths_target = torch.FloatTensor([len(sent_target)])\n                \n            bi += 1\n            if bi == len(source_processor.sentences):\n                bi = 0\n            yield (word_tens,\n                   pos_tens,\n                   tar_tens,\n                   sentence_lengths_source,\n                   sentence_lengths_target,\n                   np.ones(sentence_lengths_source[0]),\n                   np.ones(sentence_lengths_target[0]))\n    \n    else:\n        while True:\n            shuffle(idx)\n            batch_idx = [idx[i:i + batch_size] for i in range(0, len(idx) - (len(idx)%batch_size), batch_size)]\n            \n            for b_idx in batch_idx:\n                b_words_source = np.zeros([batch_size, source_processor.max_sentence_length+1])\n                b_positions_source = np.zeros([batch_size, source_processor.max_sentence_length+1])\n                b_words_target = np.zeros([batch_size, target_processor.max_sentence_length+2])\n\n                sentence_lengths_source = []\n                sentence_lengths_target = []\n                for i, bi in enumerate(b_idx):\n                    sent_source = source_processor.sentences[bi] + [END]\n                    sent_target = [START] + target_processor.sentences[bi] + [END]\n\n                    b_words_source[i, :len(sent_source)] = np.array([source_processor.w2i[w] for w in sent_source])\n                    b_positions_source[i, :len(sent_source)] = np.array([i for i in range(len(sent_source))])\n                    b_words_target[i, :len(sent_target)] = np.array([target_processor.w2i[w] for w in sent_target])\n\n                    sentence_lengths_source.append(len(sent_source))\n                    sentence_lengths_target.append(len(sent_target))\n                    \n                max_len_source = np.max(sentence_lengths_source)\n                max_len_target = np.max(sentence_lengths_target)\n                if run_gpu:\n                    word_tens = torch.from_numpy(b_words_source).type(torch.cuda.LongTensor)\n                    pos_tens = torch.from_numpy(b_positions_source).type(torch.cuda.LongTensor)\n                    tar_tens = torch.from_numpy(b_words_target).type(torch.cuda.LongTensor)\n                    sentence_lengths_source = torch.cuda.FloatTensor(sentence_lengths_source)\n                    sentence_lengths_target = torch.cuda.FloatTensor(sentence_lengths_target)\n                    source_mask = torch.from_numpy((b_words_source > 0).astype(int)).type(torch.cuda.FloatTensor)\n                    target_mask = torch.from_numpy((b_words_target > 0).astype(int)).type(torch.cuda.LongTensor)\n                else:\n                    word_tens = torch.from_numpy(b_words_source).type(torch.LongTensor)\n                    pos_tens = torch.from_numpy(b_positions_source).type(torch.LongTensor)\n                    tar_tens = torch.from_numpy(b_words_target).type(torch.LongTensor)\n                    sentence_lengths_source = torch.FloatTensor(sentence_lengths_source)\n                    sentence_lengths_target = torch.FloatTensor(sentence_lengths_target)\n                    source_mask = torch.from_numpy((b_words_source > 0).astype(int)).type(torch.FloatTensor)\n                    target_mask = torch.from_numpy((b_words_target > 0).astype(int)).type(torch.LongTensor)\n\n                yield (word_tens,\n                       pos_tens,\n                       tar_tens,\n                       sentence_lengths_source,\n                       sentence_lengths_target,\n                       source_mask,\n                       target_mask)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Sequence 2 Sequence RNN's","metadata":{}},{"cell_type":"markdown","source":"### Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, source_vocab_size, source_max_length, embeddings_dim):\n        super().__init__()        \n        self.word_embeddings = nn.Embedding(source_vocab_size, embeddings_dim)\n        self.pos_embeddings = nn.Embedding(source_max_length, embeddings_dim)\n        \n    def forward(self, words_batch, pos_batch, sentence_lengths, mask): # all inputs are tensors\n        words_emb = self.word_embeddings(words_batch)\n        pos_emb = self.pos_embeddings(pos_batch)\n        full_emb = torch.add(words_emb,pos_emb)\n        mean_emb = full_emb.mul(mask.unsqueeze(2).expand_as(full_emb)).sum(dim=1).float()\n        mean_emb = mean_emb.div(sentence_lengths.float().view(-1,1)) # batched version of torch.mean(full_emb,1)\n        return full_emb, mean_emb","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Decoder","metadata":{}},{"cell_type":"code","source":"# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n\nclass Decoder(nn.Module):\n    def __init__(self, target_vocab_size, embeddings_dim, dropout_p=0.1): # embeddings is hidden, vocab size is output size\n        super().__init__()\n        \n        self.embedding_dim = embedding_dims\n        self.dropout_p = dropout_p\n        \n        self.target_embeddings = nn.Embedding(target_vocab_size, embeddings_dim)\n        self.gru = nn.GRU(embeddings_dim*2, embeddings_dim, batch_first=True) # gru is an LSTM, and has 2 outputs\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.out = nn.Linear(embeddings_dim, target_vocab_size)\n        \n    def forward(self, gold_words_batch, hidden_batch, stacked_encoded_words_batch):\n        batch_size = gold_words_batch.size(0)\n        encoded_length = stacked_encoded_words_batch.size(1)\n        \n        emb = self.target_embeddings(gold_words_batch)\n        emb = self.dropout(emb)\n        \n        # attention            \n        alphas = torch.zeros(batch_size, encoded_length)\n        alphas = hidden_batch.view(batch_size,1,self.embedding_dim).bmm(\n            stacked_encoded_words_batch.view(batch_size,self.embedding_dim,encoded_length)) # batched version of dot product\n        \n        # Turn to probability distribution\n        alphas = F.softmax(alphas, dim=2)\n            \n        # context is weights x hidden states from encoder\n        context = torch.bmm(alphas, stacked_encoded_words_batch)     \n        del(alphas)\n        \n        # we have to concat context + emb        \n        input = torch.cat((emb, context), 2)\n        del(context)\n        \n        gru_output, hidden = self.gru(input, hidden_batch.view(1, batch_size, self.embedding_dim))\n        \n        output = self.out(gru_output)\n        output = F.log_softmax(output, dim=2)        \n        \n        return output, hidden","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Encoder Decoder","metadata":{}},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self, embeddings_dim,\n                 source_vocab_size, source_max_length, \n                 target_vocab_size, dropout_p=0.1):\n        super().__init__()\n        \n        self.source_vocab_size = source_vocab_size\n        self.target_vocab_size = target_vocab_size\n        \n        self.encoder = Encoder(source_vocab_size,\n                               source_max_length + 1, \n                               embeddings_dim)\n        self.decoder = Decoder(target_vocab_size, \n                               embeddings_dim,\n                               dropout_p=dropout_p)\n        self.loss = nn.NLLLoss(ignore_index=0)\n        if run_gpu:\n            self.encoder = self.encoder.cuda()\n            self.decoder = self.decoder.cuda()\n            self.loss = self.loss.cuda()\n\n    def forward(self, words_batch_source, pos_batch_source, sentence_length_source, words_batch_target, sentence_lengths_target, source_mask, target_mask):\n        out = 0\n        \n        # Encode\n        all_embs, hidden_state_batch = self.encoder(words_batch_source, pos_batch_source, sentence_length_source, source_mask)\n        \n#         #clean up\n#         del(words_batch_source)\n#         del(pos_batch_source)\n#         del(sentence_length_source)\n        \n        # Decode\n        batch_size,max_sent_len = words_batch_target.shape\n        for w_idx in range(max_sent_len-1):\n            prediction, hidden_state_batch = self.decoder(words_batch_target[:,w_idx].view(-1,1), \n                                                          hidden_state_batch,\n                                                          all_embs)\n            out += self.loss(prediction.view(batch_size, self.target_vocab_size), \n                             words_batch_target[:,w_idx+1].mul(target_mask[:,w_idx+1]))#apply the mask over the target\n        #cleanup\n        del(all_embs)\n        del(prediction)\n        del(hidden_state_batch)\n#         del(words_batch_target)\n        \n#         out = torch.div(out.view(batch_size,1),sentence_lengths_target.view(batch_size,1))  # the loss is the average of losses, so divide over number of words in each sentence\n#         del(sentence_lengths_target)\n        return out","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Running the Code","metadata":{}},{"cell_type":"markdown","source":"## Define Hyper Parameters","metadata":{}},{"cell_type":"markdown","source":"#### files","metadata":{}},{"cell_type":"code","source":"# Raw\ntrain_data = {'en': 'data/train/train.en', 'fr': 'data/train/train.fr'}\nval_data = {'en': 'data/val/val.en', 'fr': 'data/val/val.fr'}\ntest_data = {'en': 'data/test/test_2017_flickr.en', 'fr': 'data/test/test_2017_flickr.fr'}\n\n# Preprocessed\nsource_train_file = 'data/train/train_tokenized_lowercased_bpe.fr'\ntarget_train_file = 'data/train/train_tokenized_lowercased_bpe.en'\nsource_val_file = 'data/val/val_tokenized_lowercased_bpe.fr'\ntarget_val_file = 'data/val/val_tokenized_lowercased_bpe.en'\nsource_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.fr'\ntarget_test_file = 'data/test/test_2017_flickr_tokenized_lowercased_bpe.en'","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"#### Network Parameters","metadata":{}},{"cell_type":"code","source":"learning_rate = 1e-3\nmax_epochs = 30\nbatch_size = 64\n\nsource_vocab_size = 30000\ntarget_vocab_size = 30000\nhidden_dims = 128\nembedding_dims = 128\n\nsave_step = 100","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## process data","metadata":{}},{"cell_type":"markdown","source":"perform the preprocessing","metadata":{}},{"cell_type":"code","source":"preprocess(train_data, val_data, test_data, source_vocab_size)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"tokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/train/train.en > data/train/train_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/train/train_tokenized.en > data/train/train_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/train/train.fr > data/train/train_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/train/train_tokenized.fr > data/train/train_tokenized_lowercased.fr\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/val/val.en > data/val/val_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/val/val_tokenized.en > data/val/val_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/val/val.fr > data/val/val_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/val/val_tokenized.fr > data/val/val_tokenized_lowercased.fr\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < data/test/test_2017_flickr.en > data/test/test_2017_flickr_tokenized.en\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/test/test_2017_flickr_tokenized.en > data/test/test_2017_flickr_tokenized_lowercased.en\n\ntokenize command:\tperl tools/mosesdecoder/scripts/tokenizer/tokenizer.perl -l fr < data/test/test_2017_flickr.fr > data/test/test_2017_flickr_tokenized.fr\nlowercase command:\tperl tools/mosesdecoder/scripts/tokenizer/lowercase.perl < data/test/test_2017_flickr_tokenized.fr > data/test/test_2017_flickr_tokenized_lowercased.fr\n\nlearn vocab command:\tpython tools/subword-nmt/subword_nmt/learn_joint_bpe_and_vocab.py --input data/train/train_tokenized_lowercased.en data/train/train_tokenized_lowercased.fr -s 30000 -o data/codes.bpe --write-vocabulary data/vocab.en data/vocab.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/train/train_tokenized_lowercased.en > data/train/train_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/train/train_tokenized_lowercased.fr > data/train/train_tokenized_lowercased_bpe.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/val/val_tokenized_lowercased.en > data/val/val_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/val/val_tokenized_lowercased.fr > data/val/val_tokenized_lowercased_bpe.fr\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.en --vocabulary-threshold 50 < data/test/test_2017_flickr_tokenized_lowercased.en > data/test/test_2017_flickr_tokenized_lowercased_bpe.en\nbpe command:\tpython tools/subword-nmt/subword_nmt/apply_bpe.py -c data/codes.bpe --vocabulary data/vocab.fr --vocabulary-threshold 50 < data/test/test_2017_flickr_tokenized_lowercased.fr > data/test/test_2017_flickr_tokenized_lowercased_bpe.fr\n","output_type":"stream"}]},{"cell_type":"markdown","source":"prepare data for the model","metadata":{}},{"cell_type":"code","source":"source_processor = DataProcessor(source_train_file, source_vocab_size)\ntarget_processor = DataProcessor(target_train_file, target_vocab_size)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## setup the Network","metadata":{}},{"cell_type":"code","source":"encdec = EncoderDecoder(embedding_dims, \n                        source_processor.vocab_size,\n                        source_processor.max_sentence_length,\n                        target_processor.vocab_size)\n\nif run_gpu:\n    encdec = encdec.cuda()\n    \nopt = Adam(encdec.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training","metadata":{}},{"cell_type":"code","source":"losses = []\ncount = 0\ngen = batch_generator(source_processor, target_processor, batch_size)\nsentences_in_data = len(source_processor.sentences)\niterations = int(np.ceil(sentences_in_data/batch_size)*max_epochs)\n        \nfor it in tnrange(iterations): \n    output_i = it%save_step\n    out = 0\n    if output_i == 0:\n        if it != 0:\n            losses += output.tolist()\n            del(output)\n        if run_gpu:\n            output = torch.cuda.FloatTensor([0]*save_step)\n        else:\n            output = torch.FloatTensor([0]*save_step)\n    \n    opt.zero_grad()\n    \n    # Get the next bath of data and \n    words_batch_source, pos_batch_source, words_batch_target, sentence_lengths_source, sentence_lengths_target, source_mask, target_mask = next(gen)\n    \n    out = encdec(words_batch_source,\n                 pos_batch_source, \n                 sentence_lengths_source, \n                 words_batch_target, \n                 sentence_lengths_target, \n                 source_mask, \n                 target_mask)\n\n#     del(words_batch_source)\n#     del(pos_batch_source)\n#     del(words_batch_target)\n#     del(sentence_lengths_source)\n    \n    out.backward()\n    output[output_i] = out\n    opt.step()\n    \n    count += batch_size\n    if count >= sentences_in_data:\n        count = 0\n        # Dump trained models\n        timestamp = datetime.now()\n        torch.save(encdec.state_dict(), 'encoder_it_{}_t_{:%m_%d_%H_%M}.torchsave'.format(it, timestamp))\nlosses += output.tolist()[:output_i+1]\ndel(output)\n# Dump trained models\ntimestamp = datetime.now()\ntorch.save(encdec.state_dict(), 'encoder_last-it_{}_t_{:%m_%d_%H_%M}.torchsave'.format(it, timestamp))\n\n# TODO: Should we run until BLEU convergence on validation set? probably, if there is time","metadata":{"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, max=13620), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfbe03bce034f4d93158e17ee96ebdf"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('run_gpu:\\t\\t| '+str(run_gpu))\n# print('=====================================')\n# print('words_batch_source:\\t| ' +str(words_batch_source.is_cuda))\n# print('pos_batch_source:\\t| '   +str(pos_batch_source.is_cuda))\n# print('words_batch_target:\\t| ' +str(words_batch_target.is_cuda))\n# print('sen_len_source:\\t\\t| '   +str(sen_len_source.is_cuda))\n# print('all_embs:\\t\\t| '         +str(all_embs.is_cuda))\n# print('hidden_state_batch:\\t| ' +str(hidden_state_batch.is_cuda))\n# print('prediction:\\t\\t| '       +str(prediction.is_cuda))\n# print('output:\\t\\t\\t| '         +str(output.is_cuda))\n# print('out:\\t\\t\\t| '           +str(out.is_cuda))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses2 = [item for sublist in losses for item in sublist]\n# losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot losses","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,15))\nplt.plot(losses)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### attention visualization","metadata":{}},{"cell_type":"code","source":"# TODO\n\n# from https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb\n\ndef show_attention(input_sentence, output_words, attentions):\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n    ax.set_yticklabels([''] + output_words)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    plt.close()\n\ndef evaluate_and_show_attention(input_sentence):\n    output_words, attentions = evaluate(input_sentence)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    show_attention(input_sentence, output_words, attentions)\n    \nevaluate_and_show_attention(\"elle a cinq ans de moins que moi .\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## prediction","metadata":{}},{"cell_type":"markdown","source":"load trained models","metadata":{}},{"cell_type":"code","source":"# Uncomment to load trained models\n\nencoder = Encoder(source_processor.vocab_size,\n                  source_processor.max_sentence_length+1, \n                  embeddings_dim=embedding_dims)\ndecoder = Decoder(target_processor.vocab_size, \n                  embeddings_dim=embedding_dims)\n\ntrained_encoder_file = 'encoder_it_144999_t_05_24_16_31'\ntrained_decoder_file = 'decoder_it_144999_t_05_24_16_31'\n\nencoder.load_state_dict(torch.load(trained_encoder_file))\ndecoder.load_state_dict(torch.load(trained_decoder_file))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_id = trained_encoder_file[8:-3]\nprediction_file_name = 'test_{}.pred'.format(file_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_processor_test = DataProcessor(source_test_file, None)\ntarget_processor_test = DataProcessor(target_test_file, None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_sentences = []\n\nfor s in source_processor_test.sentences:\n    words_tokens = torch.LongTensor([source_processor_test.w2i[w] for w in s])\n    pos_tokens = torch.LongTensor([i for i in range(len(s))])\n    \n    # Encode\n    all_embs, mean_emb = encoder(words_tokens.view(1, len(s)),\n                                 pos_tokens.view(1, len(s)), \n                                 torch.FloatTensor([len(s)]))\n    \n    # Decode\n    predicted_words = []\n    \n    start_token = torch.LongTensor([target_processor.w2i[START]])\n    prediction = start_token.view(1,1)\n    \n    hidden_state_batch = mean_emb\n    \n    for w_idx in range(target_processor.max_sentence_length):# loop until EOS is produced or a max is reached (max_sentence_length)\n        prediction, hidden_state_batch = decoder(\n            prediction, # the previous prediction\n            hidden_state_batch,\n            all_embs)\n        \n        index_predicted_word = np.argmax(prediction.detach().numpy(), axis=2)[0][0]\n        predicted_word = target_processor.i2w[index_predicted_word]\n        predicted_words.append(predicted_word)\n        \n        if predicted_word == END:\n            break\n            \n        prediction = torch.LongTensor([index_predicted_word]).view(1,1)\n    \n    predicted_sentences.append(predicted_words)\n    \nwith open(prediction_file_name, 'w', encoding='utf-8') as f:\n    for p in predicted_sentences:\n        if p[-1] == END:\n            p = p[:-1]\n        f.write(' '.join(p) + '\\n')\n\nprint('Predictions ready on file: {}'.format(prediction_file_name))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"markdown","source":"Restore original segmentation","metadata":{}},{"cell_type":"code","source":"prediction_restored = prediction_file_name[:-5] + '_restored.pred'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sed -r 's/(@@ )|(@@ ?$)//g' Use the Powershell version instead. Watch for ASCII!\n\nrestore_command = 'get-content {input_file} | %{{$_ -replace \"(@@ )|(@@ ?$)\",\"\"}} | out-file {output_file} -encoding ASCII'.format(\n    input_file = prediction_file_name,\n    output_file = prediction_restored)\n\nprint('restore file command:\\t', restore_command)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BLEU","metadata":{}},{"cell_type":"code","source":"bleu_command = 'perl tools/mosesdecoder/scripts/generic/multi-bleu.perl -lc data/test/test_2017_flickr_tokenized_lowercased.en < {} > {}'.format(\n    prediction_restored,\n    'bleu_results/' + file_id + '.bleu')\n\nprint('bleu file command:\\t', bleu_command)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Meteor","metadata":{}},{"cell_type":"code","source":"meteor_command = 'java -jar tools/meteor-1.5/meteor-1.5.jar {} data/test/test_2017_flickr_tokenized_lowercased.en > {}'.format(\n    prediction_restored,\n    'meteor_results/' + file_id + '.meteor')\n\nprint('meteor command:\\t', meteor_command)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TER","metadata":{}},{"cell_type":"code","source":"# Append ids to both gold and prediction files\ngold = 'data/test/test_2017_flickr_tokenized_lowercased.en'\ngold_ter = gold[:-3] + '_ter.en'\n\nprediction_restored_ter = prediction_restored[:-5] + '_ter.pred'\n\nwith open(gold, 'r') as fi, open(gold_ter, 'w') as fo:\n    for i, line in enumerate(fi.readlines()):\n        last_char = line[-1]\n        fo.write('{} ({}){}'.format(line[:-1], i, last_char))\n\nwith open(prediction_restored, 'r') as fi, open(prediction_restored_ter, 'w') as fo:\n    for i, line in enumerate(fi.readlines()):\n        last_char = line[-1]\n        fo.write('{} ({}){}'.format(line[:-1], i, last_char))","metadata":{"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"ter_command = 'java -jar tools/tercom-0.7.25/tercom.7.25.jar -r {} -h {} -n {} > {}'.format(\n    gold_ter,\n    prediction_restored_ter,\n    'ter_results/' + file_id,\n    'ter_results/' + file_id + '_out.txt')\n\nprint('ter command:\\t', ter_command)","metadata":{"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"ter command:\t java -jar tools/tercom-0.7.25/tercom.7.25.jar -r data/test/test_2017_flickr_tokenized_lowercased_ter.en -h test_it_144999_t_05_24_16_restored_ter.pred -n ter_results/it_144999_t_05_24_16 > ter_results/it_144999_t_05_24_16_out.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### beam search","metadata":{}},{"cell_type":"code","source":"class Hypothesis(object):\n    \"\"\"use for decoding\"\"\"\n    def __init__(self, perm, score, state):\n        self.perm = perm\n        self.score = score\n        self.state = state\n\n    def update(self):\n        return\n\ndef decode(sent, beam_size):\n    \"\"\"Beam search decoder\"\"\"\n    \"\"\"bin: keep track of hypothesis, bin[i] contains all the hypotheses of length i\"\"\"\n    bin = [] # to keep track of hypothesis\n\n    # first of all, we need to encode the whole source sentence\n    # since this is done only one time\n    # map word to its id\n    fids = map(numberizer.numberize, sent[:-1])\n    enc_state = encode(fids)\n    \"\"\"\n        enc_state is a computational graph\n        it's very expensive to use it during decoding, since the graph will be recomputed each time\n        and during decoding, we don't update parameter, so we would like to fix enc_state\n        This is done nicely by compute the actual value of enc_state\n    \"\"\"\n    enc_state = constant(compute_value(enc_state))  # now it's a constant, accessing time is constant\n\n    \"\"\" the output layer also can be precomputed \"\"\"\n    out_layer = encode_birnn(fids)\n    out_layer = constant(compute_value(out_layer))  ## turn it to a precomputed big matrix\n    \"\"\" now we start the decoder with encoder state \"\"\"\n    dec.start(enc_state)\n    state0 = dec.step(embeddings[numberizer.numberize(\"<s>\")])\n    state0_const = constant(compute_value(state0))\n    # create the first hypothesis\n    h0 = Hypothesis([-1], 0.0, state0_const)\n    # put h0 to bin\n    stacks = []\n    stacks.append(h0)\n    bin.append(stacks)  # note that we put h0 in to a stack first\n\n    # now we can loop through number of source words\n    n = len(fids)\n    for i in range(n):\n        prev_stack = bin[i]\n        curr_stack = []\n        for hypo in prev_stack:\n            # expand it\n            # first, reset the decoder state\n            dec.start(hypo.state)\n            # update decoder state with the previous generated word\n            last_id = hypo.perm[-1]  # here it's just the position\n            # we get the actual word\n            word_id = fids[last_id]\n            # then turn it to a vector\n            last_inpt = embeddings[word_id]\n            # now, update decode\n            new_state = dec.step(last_inpt)\n            #compute the log output\n            log_prob = logsoftmax(dot(out_layer, new_state))\n            # actually compute it\n            log_prob = compute_value(log_prob)\n            # get out top beam_size log prob\n            \"\"\"Add your code here\"\"\"\n            for j,prob in enumerate(log_prob):\n                perm = list(hypo.perm)\n                #if not covered yet, extend the permutation\n                if(j not in perm):\n                    perm.append(j)\n                    new_prob = hypo.score + prob\n                    new_h = Hypothesis(perm,new_prob,new_state)\n                    curr_stack.append(new_h)\n\n        #if there more then 100 permutations, take the top 100\n        if len(curr_stack) > beam_size:\n            ordered_scores = []\n            #get the scores from the hypothesis\n            for h in curr_stack:\n                ordered_scores.append(h.score)\n            #get the indexs from 100 highest scores\n            locations = sorted(range(len(ordered_scores)), key=lambda i: ordered_scores[i])[-beam_size:]\n            #put top 100 in the bin\n            stacks = [curr_stack[i] for i in locations]\n            bin.append(stacks)\n        else:\n            bin.append(curr_stack)\n\n    ordered_scores = []\n    last_stack = bin[-1]\n    #get the scores from the hypothesis\n    for h in last_stack:\n        ordered_scores.append(h.score)\n\n    #return the hypothesis with highest score\n    return last_stack[ordered_scores.index(max(ordered_scores))].perm[1:]","metadata":{},"execution_count":null,"outputs":[]}]}